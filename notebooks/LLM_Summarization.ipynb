{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyOuKsr4o+HKuWpTCDORRX/S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"91c7bb3d288141c8b3342a961fe676a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60eb53920bbd4a2e8114388b1258f82f","IPY_MODEL_076325448ed84b209cc097b42bc68d7b","IPY_MODEL_c707254faa5f476b981ec803391f73f2"],"layout":"IPY_MODEL_5dd2a94e6d594d84bcc2daab0978a68d"}},"60eb53920bbd4a2e8114388b1258f82f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f076e52cfb9470baf734a5bb8feeef3","placeholder":"​","style":"IPY_MODEL_e48b710518eb4ecca2354d7da43b3fcd","value":"Map: 100%"}},"076325448ed84b209cc097b42bc68d7b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8303f8005524e7ab820e2a07c68f197","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_698ecf2029454f5c92a5a31003b28d05","value":4}},"c707254faa5f476b981ec803391f73f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d10a3c0513341a28cdc28986d552110","placeholder":"​","style":"IPY_MODEL_f7f4a2c21b0240df86e7129de9dbe1d0","value":" 4/4 [00:00&lt;00:00, 62.56 examples/s]"}},"5dd2a94e6d594d84bcc2daab0978a68d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f076e52cfb9470baf734a5bb8feeef3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e48b710518eb4ecca2354d7da43b3fcd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8303f8005524e7ab820e2a07c68f197":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"698ecf2029454f5c92a5a31003b28d05":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d10a3c0513341a28cdc28986d552110":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7f4a2c21b0240df86e7129de9dbe1d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"085d0df17c7941b9bd28f546bdb84d0f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98bd62c7563a4fd7821dc213eadb7e42","IPY_MODEL_e1f8e899eae7447ba2d31ee77e2415cd","IPY_MODEL_0dafac4d52f44e1eb36bc67818e5afe1"],"layout":"IPY_MODEL_3e0893437d4642a385d4853323206592"}},"98bd62c7563a4fd7821dc213eadb7e42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be77765c50104bf397762f99c6e1aaad","placeholder":"​","style":"IPY_MODEL_cc6b57192fe64bc39fbe28aa7a0c2a86","value":"Loading checkpoint shards: 100%"}},"e1f8e899eae7447ba2d31ee77e2415cd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6918c6c4a80499f99ea7bfbec4d6e1d","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0d0e8afb9a854d758eebf4c7255af71a","value":3}},"0dafac4d52f44e1eb36bc67818e5afe1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3f9c4ec09bc41819f803c86d06391e3","placeholder":"​","style":"IPY_MODEL_5e616f9472dd4415ac0a2504cd212540","value":" 3/3 [00:05&lt;00:00,  1.67s/it]"}},"3e0893437d4642a385d4853323206592":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be77765c50104bf397762f99c6e1aaad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc6b57192fe64bc39fbe28aa7a0c2a86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6918c6c4a80499f99ea7bfbec4d6e1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d0e8afb9a854d758eebf4c7255af71a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3f9c4ec09bc41819f803c86d06391e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e616f9472dd4415ac0a2504cd212540":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d6b5423b23f4a26b4279be7db86f228":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7214998127b74ff79a02c79842affa3b","IPY_MODEL_7fa5ac7a06dc4fd39b8f7de255531c21","IPY_MODEL_dab5e9bf4e9c4c2c89b98dba3e465c12"],"layout":"IPY_MODEL_f8cd7b1500a542ce813f12dd693a77e3"}},"7214998127b74ff79a02c79842affa3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38ef6a17f3314b2ab4290b0ac79ead0e","placeholder":"​","style":"IPY_MODEL_0bd337245bce42dbbb9e9ec36f904b6b","value":"Map: 100%"}},"7fa5ac7a06dc4fd39b8f7de255531c21":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_04cfebc047e94d99afbc7cf34bb8682a","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_141a0c0b346f4cda9e63444c19ed7f9b","value":4}},"dab5e9bf4e9c4c2c89b98dba3e465c12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1ab24fae6124414b47d3502b62de612","placeholder":"​","style":"IPY_MODEL_8ab845493bbf43f1832c6d23779410d8","value":" 4/4 [00:00&lt;00:00, 58.39 examples/s]"}},"f8cd7b1500a542ce813f12dd693a77e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38ef6a17f3314b2ab4290b0ac79ead0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd337245bce42dbbb9e9ec36f904b6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04cfebc047e94d99afbc7cf34bb8682a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"141a0c0b346f4cda9e63444c19ed7f9b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1ab24fae6124414b47d3502b62de612":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ab845493bbf43f1832c6d23779410d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["### **Cell 1: GPU Check**"],"metadata":{"id":"-JDVun5LtRI7"}},{"cell_type":"code","source":["!nvidia-smi\n","import torch, sys, platform\n","print(\"CUDA available:\", torch.cuda.is_available())\n","print(\"PyTorch:\", torch.__version__)\n","print(\"Python:\", sys.version)\n","print(\"OS:\", platform.platform())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HnjXKARts2ip","executionInfo":{"status":"ok","timestamp":1755374634812,"user_tz":-120,"elapsed":211,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"0abfd2fe-53ef-49ad-e3cd-ec6942a872cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Aug 16 20:03:54 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0             53W /  400W |   31673MiB /  40960MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n","CUDA available: True\n","PyTorch: 2.6.0+cu124\n","Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n","OS: Linux-6.1.123+-x86_64-with-glibc2.35\n"]}]},{"cell_type":"markdown","source":["### **Cell 2 — Mount Drive & set paths**"],"metadata":{"id":"xWc1ggt2tcmk"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# === change these to your folders ===\n","DATA_DIR = \"/content/drive/MyDrive/summary_generation_data\"   # your JSONLs live here\n","SAVE_ROOT = \"/content/drive/MyDrive/model_weights_tokens_files\"   # parent folder for all runs\n","# ====================================\n","\n","# Versioning: time-stamped run folder\n","import os, time\n","RUN_ID = time.strftime(\"%Y%m%d-%H%M%S\")  # e.g., 20250816-142501\n","SAVE_DIR_RUN = os.path.join(SAVE_ROOT, f\"mistral7b_fp16_{RUN_ID}\")\n","os.makedirs(SAVE_DIR_RUN, exist_ok=True)\n","\n","COMM_PATH = f\"{DATA_DIR}/commentary.jsonl\"\n","SC_PATH   = f\"{DATA_DIR}/scorecards.jsonl\"\n","RPT_PATH  = f\"{DATA_DIR}/reports.jsonl\"\n","\n","print(\"Data dir:\", DATA_DIR)\n","print(\"Run output dir:\", SAVE_DIR_RUN)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81qKa8YSusEE","executionInfo":{"status":"ok","timestamp":1755374640134,"user_tz":-120,"elapsed":1101,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"c0803678-d41f-4452-a7ca-e1daf7f531dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Data dir: /content/drive/MyDrive/summary_generation_data\n","Run output dir: /content/drive/MyDrive/model_weights_tokens_files/mistral7b_fp16_20250816-200400\n"]}]},{"cell_type":"markdown","source":["### **Cell 3: Requiremets & Dependencies Install**"],"metadata":{"id":"YJajOdQEtqZS"}},{"cell_type":"code","source":["# Keep it minimal to avoid conflicts\n","!pip -q install --no-cache-dir \\\n","  \"transformers==4.43.3\" \\\n","  \"peft==0.12.0\" \\\n","  \"accelerate==0.31.0\" \\\n","  \"datasets==2.20.0\" \\\n","  \"sentence-transformers==3.0.1\"\n","\n","import transformers, peft, accelerate, datasets, sentence_transformers\n","print(\"transformers:\", transformers.__version__)\n","print(\"peft:\", peft.__version__)\n","print(\"accelerate:\", accelerate.__version__)\n","print(\"datasets:\", datasets.__version__)\n","print(\"sentence-transformers:\", sentence_transformers.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q1Y6qapTu2iL","executionInfo":{"status":"ok","timestamp":1755374654417,"user_tz":-120,"elapsed":2729,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"1ef20926-f3c9-4886-ccfc-64f885132223"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["transformers: 4.43.3\n","peft: 0.12.0\n","accelerate: 0.31.0\n","datasets: 2.20.0\n","sentence-transformers: 3.0.1\n"]}]},{"cell_type":"markdown","source":["### **Cell 4: model & sequence lengths**"],"metadata":{"id":"M2Uyrs1yt8NG"}},{"cell_type":"code","source":["# Recommended:\n","MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","# Alternative:\n","# MODEL_ID = \"google/gemma-7b-it\"  # accept license on HF first if needed\n","\n","# Sequence lengths (A100 can handle fp16 with these)\n","MAX_IN  = 2048\n","MAX_OUT = 256\n","\n","print(\"MODEL_ID:\", MODEL_ID)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-cfnJWxTu9Zx","executionInfo":{"status":"ok","timestamp":1755374735010,"user_tz":-120,"elapsed":7,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"9f078723-6c16-4e2d-c963-4a1e5bce6137"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MODEL_ID: mistralai/Mistral-7B-Instruct-v0.3\n"]}]},{"cell_type":"markdown","source":["### **Cell 5: For Gemma only login if needed**"],"metadata":{"id":"xEnMVjl2uBkZ"}},{"cell_type":"code","source":["if MODEL_ID.startswith(\"google/\"):\n","    from huggingface_hub import login\n","    print(\"Gemma selected — if you get 403 later, run login() and paste your HF token.\")\n","    # login()  # uncomment if needed"],"metadata":{"id":"1aj6js11vXhb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Cell 6: Load JSONLs & build examples**"],"metadata":{"id":"xTstqOCmuLti"}},{"cell_type":"code","source":["# Cell 6 — build examples with a REQUIRED deterministic opening sentence\n","\n","import os, json, re\n","\n","def load_jsonl(path):\n","    d = {}\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            o = json.loads(line)\n","            d[o[\"match_id\"]] = o\n","    return d\n","\n","assert os.path.exists(COMM_PATH), f\"Missing {COMM_PATH}\"\n","assert os.path.exists(SC_PATH),   f\"Missing {SC_PATH}\"\n","assert os.path.exists(RPT_PATH),  f\"Missing {RPT_PATH}\"\n","\n","commentary = load_jsonl(COMM_PATH)\n","scorecards = load_jsonl(SC_PATH)\n","reports    = load_jsonl(RPT_PATH)\n","\n","match_ids = sorted(set(commentary) & set(scorecards) & set(reports))\n","print(\"Matches available:\", len(match_ids))\n","print(\"Example match_id:\", match_ids[0] if match_ids else None)\n","\n","def compact_stats(stats):\n","    keep = [\"team1\",\"team2\",\"winner\",\"result\",\"result_margin\",\"venue\",\"date\",\n","            \"toss_winner\",\"toss_decision\",\"first_innings_runs\",\"first_innings_wkts\",\n","            \"second_innings_runs\",\"second_innings_wkts\",\"top_batters\",\"top_bowlers\"]\n","    s = {k: stats.get(k) for k in keep if k in stats}\n","    if s.get(\"top_batters\"): s[\"top_batters\"] = s[\"top_batters\"][:2]\n","    if s.get(\"top_bowlers\"): s[\"top_bowlers\"] = s[\"top_bowlers\"][:2]\n","    return s\n","\n","KEYWORDS = (\"wicket\",\"out\",\"caught\",\"lbw\",\"review\",\"drs\",\"six\",\"four\",\"powerplay\",\"death\",\"target\",\"needed\",\"fifty\",\"hundred\")\n","\n","def pick_chunks(chunks, k=6):\n","    if len(chunks) <= k: return chunks\n","    def score(s):\n","        t = s.lower()\n","        return sum(kw in t for kw in KEYWORDS)\n","    half = k//2\n","    head_tail = chunks[:half] + chunks[-half:]\n","    middle = chunks[half:-half] if len(chunks) > k else []\n","    best_mid = sorted(middle, key=score, reverse=True)[:max(0, k-len(head_tail))]\n","    pos = {c:i for i,c in enumerate(chunks)}\n","    merged = sorted(set(head_tail + best_mid), key=lambda c: pos[c])\n","    return merged[:k]\n","\n","def loser_of(stats):\n","    t1, t2, w = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\"), stats.get(\"winner\",\"\")\n","    if not w: return \"\"\n","    return t2 if w == t1 else t1\n","\n","def opening_sentence(stats):\n","    w = stats.get(\"winner\",\"\")\n","    l = loser_of(stats)\n","    m = str(stats.get(\"result_margin\",\"\"))\n","    v = stats.get(\"venue\",\"\")\n","    if not (w and l and m and v):\n","        return \"\"\n","    return f\"{w} defeated {l} by {m} at {v}.\"\n","\n","def build_example(mid, max_chunks=6):\n","    chunks = pick_chunks(commentary[mid][\"commentary_chunks\"], max_chunks)\n","    stats  = compact_stats(scorecards[mid][\"stats\"])\n","    req_open = opening_sentence(stats)  # deterministic opener from facts\n","\n","    prompt = (\n","        \"You are a cricket Expert. Produce a concise IPL match report with exactly 3 paragraphs:\\n\"\n","        \"P1: REQUIRED opening sentence (copy facts) + one sentence context.\\n\"\n","        \"P2: Turning events that decided the game.\\n\"\n","        \"P3: Standout batter & bowler; short details (toss/DRS/injuries/pitch) + closing.\\n\\n\"\n","        \"Rules:\\n\"\n","        \"- 180–220 words total.\\n\"\n","        \"- Use only SCORECARD & EXCERPTS; do NOT invent facts.\\n\"\n","        '- REQUIRED opening sentence format: \"{winner} defeated {loser} by {result_margin} at {venue}.\"\\n\\n'\n","        f\"SCORECARD:\\n{json.dumps(stats, ensure_ascii=False)}\\n\\n\"\n","        \"COMMENTARY EXCERPTS (chronological):\\n\" + \"\\n\\n\".join(chunks) + \"\\n\\n\"\n","        \"Write the report now.\"\n","    )\n","\n","    gold = reports[mid][\"report_text\"].strip()\n","    if req_open and not gold.startswith(req_open):\n","        gold = f\"{req_open}\\n\\n\" + gold\n","\n","    return {\"input\": prompt, \"output\": gold}\n","\n","examples = [build_example(mid) for mid in match_ids]\n","len(examples), examples[0][\"input\"][:300]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3egdVDt3vZgN","executionInfo":{"status":"ok","timestamp":1755374743437,"user_tz":-120,"elapsed":34,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"35f27fb5-d94f-4d11-a068-284833ccffc0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Matches available: 4\n","Example match_id: 1422119\n"]},{"output_type":"execute_result","data":{"text/plain":["(4,\n"," 'You are a cricket Expert. Produce a concise IPL match report with exactly 3 paragraphs:\\nP1: REQUIRED opening sentence (copy facts) + one sentence context.\\nP2: Turning events that decided the game.\\nP3: Standout batter & bowler; short details (toss/DRS/injuries/pitch) + closing.\\n\\nRules:\\n- 180–220 word')"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["### **Cell 7: Tokenize (supervised fine-tuning format)**"],"metadata":{"id":"9ACVC_jyuUUW"}},{"cell_type":"code","source":["# Cell 7 — tokenize for SFT (input + eos + output, mask input in labels)\n","\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","\n","tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","def to_features(ex, max_in=2048, max_out=256):\n","    x = tok(ex[\"input\"], truncation=True, max_length=max_in)\n","    y = tok(ex[\"output\"], truncation=True, max_length=max_out)\n","    input_ids = x[\"input_ids\"] + [tok.eos_token_id] + y[\"input_ids\"]\n","    attention = [1] * len(input_ids)\n","    labels    = [-100] * (len(x[\"input_ids\"]) + 1) + y[\"input_ids\"]\n","    input_ids = input_ids[:max_in]\n","    attention = attention[:max_in]\n","    labels    = labels[:max_in]\n","    return {\"input_ids\": input_ids, \"attention_mask\": attention, \"labels\": labels}\n","\n","raw_ds = Dataset.from_list(examples)\n","ds = raw_ds.map(lambda row: to_features(row, MAX_IN, MAX_OUT), remove_columns=[\"input\",\"output\"])\n","ds"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["91c7bb3d288141c8b3342a961fe676a6","60eb53920bbd4a2e8114388b1258f82f","076325448ed84b209cc097b42bc68d7b","c707254faa5f476b981ec803391f73f2","5dd2a94e6d594d84bcc2daab0978a68d","4f076e52cfb9470baf734a5bb8feeef3","e48b710518eb4ecca2354d7da43b3fcd","e8303f8005524e7ab820e2a07c68f197","698ecf2029454f5c92a5a31003b28d05","1d10a3c0513341a28cdc28986d552110","f7f4a2c21b0240df86e7129de9dbe1d0"]},"id":"78IxP8qFvbF9","executionInfo":{"status":"ok","timestamp":1755374749064,"user_tz":-120,"elapsed":592,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"f415f4d6-46dc-4bc9-f9a6-84149ca1a38b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/4 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91c7bb3d288141c8b3342a961fe676a6"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'attention_mask', 'labels'],\n","    num_rows: 4\n","})"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# Cell X — free GPU memory from previous runs\n","import gc, torch, inspect, sys\n","\n","names_to_drop = [\"gen_pipe\",\"pipe\",\"trainer\",\"model\",\"base_model\"]\n","for n in names_to_drop:\n","    if n in globals():\n","        try:\n","            obj = globals().pop(n)\n","            del obj\n","        except:\n","            pass\n","\n","gc.collect()\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","    torch.cuda.ipc_collect()\n","\n","# Optional: print free/total memory\n","if torch.cuda.is_available():\n","    free, total = torch.cuda.mem_get_info()\n","    print(f\"CUDA mem free: {free/1e9:.2f} GB / {total/1e9:.2f} GB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r4s9zG9GguBg","executionInfo":{"status":"ok","timestamp":1755375135712,"user_tz":-120,"elapsed":735,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"40945841-7587-4181-b48a-fa2cdcff170b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA mem free: 37.50 GB / 42.47 GB\n"]}]},{"cell_type":"markdown","source":["### **Cell 8: Load base model (FP16) + attach LoRA**"],"metadata":{"id":"jEnjYfR-uYIP"}},{"cell_type":"code","source":["# Cell 8 — load base model on GPU (FP16, no offload) + attach LoRA\n","\n","import os, torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import LoraConfig, get_peft_model\n","\n","# Reduce fragmentation in PyTorch allocator\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:256\"\n","\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","\n","tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","# Load on CPU first (materialized), then move to GPU\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_ID,\n","    torch_dtype=torch.float16,\n","    low_cpu_mem_usage=False,   # prevent meta tensors\n","    device_map=None,           # load on CPU\n","    attn_implementation=\"sdpa\"\n",")\n","\n","# Move whole model to GPU\n","base_model.to(\"cuda\", dtype=torch.float16)\n","\n","# Memory savers for training\n","base_model.gradient_checkpointing_enable()\n","base_model.config.use_cache = False\n","\n","# LoRA config\n","target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n","lora_cfg = LoraConfig(\n","    r=16, lora_alpha=16, lora_dropout=0.05,\n","    target_modules=target_modules, bias=\"none\", task_type=\"CAUSAL_LM\"\n",")\n","model = get_peft_model(base_model, lora_cfg)\n","\n","# Sanity: ensure nothing is on 'meta' or CPU\n","bad_meta = [n for n,p in model.named_parameters() if getattr(p, \"device\", None).type == \"meta\"]\n","bad_cpu  = [n for n,p in model.named_parameters() if getattr(p, \"device\", None).type == \"cpu\"]\n","assert not bad_meta, f\"Meta tensors detected: {bad_meta[:5]}\"\n","assert not bad_cpu,  f\"CPU tensors detected: {bad_cpu[:5]}\"\n","\n","model.print_trainable_parameters()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["085d0df17c7941b9bd28f546bdb84d0f","98bd62c7563a4fd7821dc213eadb7e42","e1f8e899eae7447ba2d31ee77e2415cd","0dafac4d52f44e1eb36bc67818e5afe1","3e0893437d4642a385d4853323206592","be77765c50104bf397762f99c6e1aaad","cc6b57192fe64bc39fbe28aa7a0c2a86","b6918c6c4a80499f99ea7bfbec4d6e1d","0d0e8afb9a854d758eebf4c7255af71a","f3f9c4ec09bc41819f803c86d06391e3","5e616f9472dd4415ac0a2504cd212540"]},"id":"qWA5rjHYvjZD","executionInfo":{"status":"ok","timestamp":1755375177815,"user_tz":-120,"elapsed":10130,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"4ffe2a22-be1a-4d4e-fd9e-75d526c4232d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"085d0df17c7941b9bd28f546bdb84d0f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["trainable params: 41,943,040 || all params: 7,289,966,592 || trainable%: 0.5754\n"]}]},{"cell_type":"markdown","source":["### **Cell 9: Train Model (short & safe defaults)**"],"metadata":{"id":"62aemYYWuc4q"}},{"cell_type":"code","source":["# Cell 9 — Build training dataset with concise 2-paragraph format, train, save\n","\n","from transformers import TrainingArguments, Trainer\n","from datasets import Dataset\n","import os, json, re\n","\n","# ---------- helpers (reuse from earlier cells if already defined) ----------\n","def loser_of(stats):\n","    t1, t2, w = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\"), stats.get(\"winner\",\"\")\n","    if not w: return \"\"\n","    return t2 if w == t1 else t1\n","\n","def format_margin(stats):\n","    margin = stats.get(\"result_margin\")\n","    res_txt = (stats.get(\"result\") or \"\").lower()\n","    if margin is None: return \"\"\n","    try:\n","        n = int(margin)\n","        unit = \"wickets\" if \"wicket\" in res_txt else (\"runs\" if \"run\" in res_txt else \"\")\n","        return f\"{n} {unit}\".strip()\n","    except Exception:\n","        return str(margin)\n","\n","def opening_sentence(stats):\n","    w = stats.get(\"winner\",\"\")\n","    l = loser_of(stats)\n","    m = format_margin(stats)\n","    v = stats.get(\"venue\",\"\")\n","    if not (w and l and m and v): return \"\"\n","    return f\"{w} defeated {l} by {m} at {v}.\"\n","\n","# If you already have these from earlier cells, this will just reuse them:\n","# - commentary, scorecards, reports, match_ids\n","# - compact_stats(stats)\n","# - pick_chunks(chunks, k)\n","# - tok tokenizer from Cell 8\n","# - MAX_IN, MAX_OUT from earlier cells (fallback values below)\n","try:\n","    MAX_IN\n","except NameError:\n","    MAX_IN = 2048\n","try:\n","    MAX_OUT\n","except NameError:\n","    MAX_OUT = 256\n","\n","# ---------- build training examples in the *new style* ----------\n","def build_training_example(mid, max_chunks=6):\n","    chunks = pick_chunks(commentary[mid][\"commentary_chunks\"], max_chunks)\n","    stats  = compact_stats(scorecards[mid][\"stats\"])\n","    # Deterministic opener from facts\n","    req_open = opening_sentence(stats)\n","\n","    # Gold summary\n","    gold = reports[mid][\"report_text\"].strip()\n","\n","    # If gold doesn't start with the deterministic opener, prepend it so the model learns the pattern\n","    if req_open and not gold.startswith(req_open):\n","        gold = f\"{req_open}\\n\\n{gold}\"\n","\n","    # Training prompt (fan-style, 2 short paragraphs, <=6 sentences, end with <END>)\n","    prompt = (\n","        \"You are a cricket expert. Write a match summary in exactly two short paragraphs (no headings).\\n\"\n","        \"Paragraph 1: Describe the main turning events that decided the game (key wickets, overs, partnerships).\\n\"\n","        \"Paragraph 2: Name one standout batter and one standout bowler; add brief pitch/toss/DRS notes if relevant; \"\n","        \"close with a one-line implication.\\n\\n\"\n","        \"Rules:\\n\"\n","        \"- Maximum 6 sentences total across both paragraphs.\\n\"\n","        \"- Use only facts from SCORECARD and COMMENTARY EXCERPTS. Never invent or exaggerate.\\n\"\n","        \"- Keep the style factual, concise, and fan-friendly. No filler like 'respectable total' or season stats.\\n\"\n","        \"- End with the token <END>.\\n\\n\"\n","        f\"SCORECARD:\\n{json.dumps(stats, ensure_ascii=False)}\\n\\n\"\n","        \"COMMENTARY EXCERPTS:\\n\" + \"\\n\".join(chunks) + \"\\n\\n\"\n","        \"### TARGET SUMMARY:\"\n","    )\n","\n","    target = gold + \" <END>\"\n","    return {\"input\": prompt, \"output\": target}\n","\n","train_examples = [build_training_example(mid, max_chunks=6) for mid in match_ids]\n","print(\"Training samples:\", len(train_examples))\n","print(train_examples[0][\"input\"][:300])\n","\n","# ---------- tokenize to SFT format (mask prompt in labels) ----------\n","def to_features(ex, max_in=MAX_IN, max_out=MAX_OUT):\n","    x = tok(ex[\"input\"], truncation=True, max_length=max_in)\n","    y = tok(ex[\"output\"], truncation=True, max_length=max_out)\n","    input_ids = x[\"input_ids\"] + [tok.eos_token_id] + y[\"input_ids\"]\n","    attention = [1] * len(input_ids)\n","    labels    = [-100] * (len(x[\"input_ids\"]) + 1) + y[\"input_ids\"]\n","    # final truncation\n","    input_ids = input_ids[:max_in]\n","    attention = attention[:max_in]\n","    labels    = labels[:max_in]\n","    return {\"input_ids\": input_ids, \"attention_mask\": attention, \"labels\": labels}\n","\n","ds = Dataset.from_list(train_examples).map(\n","    lambda ex: to_features(ex, MAX_IN, MAX_OUT),\n","    remove_columns=[\"input\", \"output\"]\n",")\n","print(ds)\n","\n","# ---------- training ----------\n","args = TrainingArguments(\n","    output_dir=\"/content/lora_out\",     # scratch (local)\n","    learning_rate=2e-4,\n","    num_train_epochs=2,                 # set 2–3 for final\n","    # max_steps=300,                    # <- uncomment to cap cost/time during trials\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=8,\n","    logging_steps=20,\n","    save_steps=300,\n","    warmup_ratio=0.05,\n","    lr_scheduler_type=\"cosine\",\n","    fp16=True,                          # FP16 on A100\n","    report_to=\"none\",\n","    max_grad_norm=0.5,\n","    ddp_find_unused_parameters=False,   # safer with LoRA\n",")\n","\n","trainer = Trainer(model=model, args=args, train_dataset=ds)\n","trainer.train()\n","\n","# ---------- save to versioned run folder ----------\n","os.makedirs(SAVE_DIR_RUN, exist_ok=True)\n","model.save_pretrained(SAVE_DIR_RUN)\n","tok.save_pretrained(SAVE_DIR_RUN)\n","print(\"Saved to:\", SAVE_DIR_RUN)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331,"referenced_widgets":["3d6b5423b23f4a26b4279be7db86f228","7214998127b74ff79a02c79842affa3b","7fa5ac7a06dc4fd39b8f7de255531c21","dab5e9bf4e9c4c2c89b98dba3e465c12","f8cd7b1500a542ce813f12dd693a77e3","38ef6a17f3314b2ab4290b0ac79ead0e","0bd337245bce42dbbb9e9ec36f904b6b","04cfebc047e94d99afbc7cf34bb8682a","141a0c0b346f4cda9e63444c19ed7f9b","e1ab24fae6124414b47d3502b62de612","8ab845493bbf43f1832c6d23779410d8"]},"id":"bHRdXWf0vman","executionInfo":{"status":"ok","timestamp":1755377335605,"user_tz":-120,"elapsed":9080,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"4500e128-0ddd-492a-a35d-76c61f1fed22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training samples: 4\n","You are a cricket expert. Write a match summary in exactly two short paragraphs (no headings).\n","Paragraph 1: Describe the main turning events that decided the game (key wickets, overs, partnerships).\n","Paragraph 2: Name one standout batter and one standout bowler; add brief pitch/toss/DRS notes if rele\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/4 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d6b5423b23f4a26b4279be7db86f228"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset({\n","    features: ['input_ids', 'attention_mask', 'labels'],\n","    num_rows: 4\n","})\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2/2 00:04, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved to: /content/drive/MyDrive/model_weights_tokens_files/mistral7b_fp16_20250816-200400\n"]}]},{"cell_type":"code","source":["# Cell X — free trainer/optimizer state so inference fits in VRAM\n","import gc, torch\n","\n","# Drop Trainer to free optimizer states & dataloaders\n","if 'trainer' in globals():\n","    try:\n","        del trainer\n","    except:\n","        pass\n","\n","gc.collect()\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","    torch.cuda.ipc_collect()\n","\n","# Switch model to eval & enable cache for faster decoding\n","model.eval()\n","if hasattr(model, \"config\"):\n","    model.config.use_cache = True"],"metadata":{"id":"9z8rxz06jEkK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Cell 10: Inference (structured prompting)**"],"metadata":{"id":"vLq6WCjxurVd"}},{"cell_type":"code","source":["pip install rapidfuzz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T338rPzTrJFN","executionInfo":{"status":"ok","timestamp":1755377877996,"user_tz":-120,"elapsed":6058,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"c6e4e725-2a9c-42c8-d654-8adf271f7efe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rapidfuzz\n","  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz\n","Successfully installed rapidfuzz-3.13.0\n"]}]},{"cell_type":"code","source":["# Cell 10 — inference (no pipeline), with delayed stop + logits constraints + whitelist sanitizers + debug saves\n","\n","import os, re, json, torch\n","from collections import Counter\n","from transformers import (\n","    StoppingCriteria, StoppingCriteriaList, LogitsProcessorList,\n","    NoBadWordsLogitsProcessor, InfNanRemoveLogitsProcessor\n",")\n","\n","# ---- optional fuzzy matching (improves name correction) ----\n","try:\n","    from rapidfuzz import process as fuzz_process, fuzz\n","    HAVE_FUZZ = True\n","except Exception:\n","    HAVE_FUZZ = False\n","\n","# ---------- base helpers ----------\n","def loser_of(stats):\n","    t1, t2, w = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\"), stats.get(\"winner\",\"\")\n","    if not w: return \"\"\n","    return t2 if w == t1 else t1\n","\n","def format_margin(stats):\n","    margin = stats.get(\"result_margin\")\n","    res_txt = (stats.get(\"result\") or \"\").lower()\n","    if margin is None: return \"\"\n","    try:\n","        n = int(margin)\n","        unit = \"wickets\" if \"wicket\" in res_txt else (\"runs\" if \"run\" in res_txt else \"\")\n","        return f\"{n} {unit}\".strip()\n","    except Exception:\n","        return str(margin)\n","\n","def opening_sentence(stats):\n","    w = stats.get(\"winner\",\"\")\n","    l = loser_of(stats)\n","    m = format_margin(stats)\n","    v = stats.get(\"venue\",\"\")\n","    if not (w and l and m and v): return \"\"\n","    return f\"{w} defeated {l} by {m} at {v}.\"\n","\n","def contains_literal(text, value):\n","    if not value: return True\n","    t = re.sub(r\"\\s+\", \" \", text).lower()\n","    v = re.sub(r\"\\s+\", \" \", str(value)).lower()\n","    return v in t\n","\n","def clean_headings(txt: str) -> str:\n","    # strip any headings that slip through\n","    txt = re.sub(r\"(?m)^\\s*(P\\d:|Paragraph\\s*\\d:|Pargraph\\s*\\d:)\\s*\", \"\", txt)\n","    txt = re.sub(r\"\\s+\\.\", \".\", txt)\n","    txt = re.sub(r\"\\s+,\", \",\", txt)\n","    return txt.strip()\n","\n","# ---------- whitelist + sanitizers ----------\n","NAME_RX = re.compile(r\"[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\")\n","\n","_GENERIC_BAD = {\n","    \"Chennai\",\"Super\",\"Kings\",\"Royal\",\"Challengers\",\"Bengaluru\",\"Bangalore\",\n","    \"Stadium\",\"Chepauk\",\"Match\",\"Overs\",\"Runs\",\"Wickets\",\"Powerplay\",\n","    \"Review\",\"DRS\",\"Paragraph\",\"Pargraph\",\"Section\",\"Heading\",\"Outline\"\n","}\n","\n","def extract_names_from_text(txt: str):\n","    cands = [m.group(0).strip() for m in NAME_RX.finditer(txt)]\n","    return [c for c in cands if c not in _GENERIC_BAD and len(c) <= 30]\n","\n","def build_player_whitelist(stats, chunks):\n","    allowed = set()\n","    for key in (\"top_batters\",\"top_bowlers\",\"players\",\"batting_card\",\"bowling_card\"):\n","        val = stats.get(key)\n","        if isinstance(val, list):\n","            for item in val:\n","                if isinstance(item, dict):\n","                    for k in (\"name\",\"player\",\"batter\",\"bowler\"):\n","                        if k in item and item[k]:\n","                            allowed.add(str(item[k]).strip())\n","                elif isinstance(item, str):\n","                    allowed.add(item.strip())\n","    counter = Counter()\n","    for c in chunks:\n","        for nm in extract_names_from_text(c):\n","            counter[nm] += 1\n","    for nm, cnt in counter.items():\n","        if cnt >= 2:\n","            allowed.add(nm)\n","    return sorted({re.sub(r\"\\s+\", \" \", a).strip() for a in allowed})\n","\n","def fuzzy_fix_name(name, whitelist):\n","    if not HAVE_FUZZ or not whitelist:\n","        return name\n","    cand, score, _ = fuzz_process.extractOne(name, whitelist, scorer=fuzz.WRatio)\n","    return cand if score >= 88 else name\n","\n","def sanitize_names(text, whitelist):\n","    if not whitelist:\n","        return text\n","    wl_lower = {w.lower(): w for w in whitelist}\n","    lines = []\n","    for line in text.splitlines():\n","        fixed = line\n","        # correct each detected name\n","        for n in set(extract_names_from_text(line)):\n","            rep = wl_lower.get(n.lower(), None)\n","            if rep is None:\n","                rep = fuzzy_fix_name(n, whitelist)\n","            if rep != n:\n","                fixed = re.sub(rf\"\\b{re.escape(n)}\\b\", rep, fixed)\n","        # drop lines that still contain OOV names AND assert actions\n","        oov = [n for n in extract_names_from_text(fixed) if n.lower() not in wl_lower]\n","        if oov and re.search(r\"\\b(dismissed|bowled|caught|lbw|stumped|scored|hit|took|figures|overs|partnership)\\b\", fixed.lower()):\n","            continue\n","        lines.append(fixed)\n","    return \"\\n\".join(lines).strip()\n","\n","def valid_over_notation(text):\n","    # Normalize X.Y overs so that Y∈{0..5}\n","    return re.sub(r\"(\\b\\d+)\\.(\\d{1,2})\\s*overs\", lambda m: f\"{m.group(1)}.{min(int(m.group(2)),5)} overs\", text)\n","\n","def sanitize_numbers(text, stats):\n","    # Bowler max overs T20 = 4\n","    txt = re.sub(r\"(\\b4)\\.(\\d)\\s*overs\", r\"\\1 overs\", text)\n","    # De-fluff\n","    txt = re.sub(r\"\\brespectable total\\b\", \"a total\", txt, flags=re.I)\n","    res = (stats.get(\"result\") or \"\").lower()\n","    if \"wicket\" in res:\n","        txt = re.sub(r\"\\bfell short\\b.*?(\\.|\\n)\", \". \", txt, flags=re.I)\n","    return valid_over_notation(txt)\n","\n","def enforce_two_paragraphs(text):\n","    paras = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n","    return \"\\n\\n\".join(paras[:3])  # opener + 2\n","\n","# ---------- generation controls (delayed stop) ----------\n","class StopOnTokensAfterMin(StoppingCriteria):\n","    def __init__(self, stop_ids, prompt_len, min_new_tokens):\n","        super().__init__()\n","        self.stop_ids = stop_ids\n","        self.prompt_len = prompt_len\n","        self.min_new_tokens = min_new_tokens\n","    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n","        gen_len = input_ids.shape[-1] - self.prompt_len\n","        if gen_len < self.min_new_tokens:\n","            return False\n","        S = self.stop_ids.shape[-1]\n","        if input_ids.shape[-1] < S:\n","            return False\n","        tail = input_ids[0, -S:]\n","        return torch.equal(tail.cpu(), self.stop_ids.cpu())\n","\n","def make_stop_criteria_after_min(tokenizer, stop_str: str, prompt_len: int, min_new_tokens: int):\n","    stop_ids = tokenizer(stop_str, add_special_tokens=False, return_tensors=\"pt\").input_ids[0]\n","    return StoppingCriteriaList([StopOnTokensAfterMin(stop_ids, prompt_len, min_new_tokens)])\n","\n","def build_bad_words_ids(tokenizer, words_or_phrases):\n","    ids = []\n","    for w in words_or_phrases:\n","        toks = tokenizer(w, add_special_tokens=False).input_ids\n","        if toks: ids.append(toks)\n","    return ids\n","\n","def contradiction_phrases(stats):\n","    res = (stats.get(\"result\") or \"\").lower()\n","    if \"wicket\" in res:\n","        return [\"fell short\", \"fell just short\", \"could not chase\", \"defended the total\",\n","                \"won by runs\", \"victory by runs\", \"ran out of overs\"]\n","    if \"run\" in res:\n","        return [\"won by wickets\", \"victory by wickets\", \"chased down comfortably\",\n","                \"reached the target\", \"got over the line in the chase\"]\n","    return []\n","\n","# ---------- main inference ----------\n","def infer_for_match(mid, max_chunks=4, save=True):\n","    chunks = pick_chunks(commentary[mid][\"commentary_chunks\"], max_chunks)\n","    stats  = compact_stats(scorecards[mid][\"stats\"])\n","    req_open = opening_sentence(stats) or \"\"\n","    whitelist = build_player_whitelist(stats, chunks)\n","\n","    # Prompt: concise, two paragraphs, no headings/filler, finish with <END>\n","    prompt = (\n","        \"You are a cricket expert. Continue the match summary in exactly two short paragraphs (no headings).\\n\"\n","        \"The REQUIRED opening sentence has already been written. Do NOT repeat it.\\n\\n\"\n","        \"Rules:\\n\"\n","        \"- Maximum 6 sentences total across both paragraphs.\\n\"\n","        \"- Use only facts from SCORECARD and COMMENTARY EXCERPTS. Never invent.\\n\"\n","        \"- Keep it fan-friendly and factual. Do not include season-wide claims unless present in SCORECARD.\\n\"\n","        \"- Do not write headings or labels.\\n\"\n","        \"- End with <END>.\\n\\n\"\n","        f\"SCORECARD:\\n{json.dumps(stats, ensure_ascii=False)}\\n\\n\"\n","        \"COMMENTARY EXCERPTS:\\n\" + \"\\n\".join(chunks) + \"\\n\\n\"\n","        \"Continue with the two paragraphs only.\"\n","    )\n","\n","    enc = tok(prompt, return_tensors=\"pt\")\n","    input_ids = enc.input_ids.to(\"cuda\")\n","    attention_mask = enc.attention_mask.to(\"cuda\")\n","\n","    banned = [\n","        \"P1:\", \"P2:\", \"P3:\", \"Paragraph 1:\", \"Paragraph 2:\", \"Paragraph 3:\",\n","        \"Pargraph 1:\", \"Pargraph 2:\", \"Section\", \"Heading\", \"Outline:\"\n","    ]\n","    for p in contradiction_phrases(stats):\n","        banned.append(p)\n","\n","    bad_words_ids = build_bad_words_ids(tok, banned)\n","\n","    logits_processors = LogitsProcessorList([\n","        InfNanRemoveLogitsProcessor(),\n","        NoBadWordsLogitsProcessor(bad_words_ids=bad_words_ids, eos_token_id=tok.eos_token_id),\n","    ])\n","\n","    MIN_NEW = 180  # keep in sync with gen_kwargs\n","    stop_criteria = make_stop_criteria_after_min(tok, \"<END>\", prompt_len=input_ids.shape[-1], min_new_tokens=MIN_NEW)\n","\n","    gen_kwargs = dict(\n","        do_sample=False,\n","        repetition_penalty=1.02,\n","        no_repeat_ngram_size=4,\n","        min_new_tokens=MIN_NEW,\n","        max_new_tokens=260,\n","        pad_token_id=tok.pad_token_id,\n","        use_cache=True,\n","        eos_token_id=None,              # don't stop on EOS\n","        logits_processor=logits_processors,\n","        stopping_criteria=stop_criteria,\n","    )\n","\n","    with torch.no_grad():\n","        out_ids = model.generate(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            **gen_kwargs,\n","        )\n","\n","    # remove prompt (no echo) and cut at <END>\n","    prompt_len = input_ids.shape[-1]\n","    cont_ids = out_ids[0, prompt_len:]\n","    raw_out = tok.decode(cont_ids, skip_special_tokens=True)\n","    raw_out = raw_out.split(\"<END>\")[0].strip()\n","    raw_out = clean_headings(raw_out)\n","\n","    # --- assemble full report (raw) ---\n","    raw_report = req_open + (\"\\n\\n\" if req_open else \"\") + raw_out\n","\n","    # --- sanitize with whitelist & numbers ---\n","    san_report = sanitize_names(raw_report, whitelist)\n","    san_report = sanitize_numbers(san_report, stats)\n","    final_report = enforce_two_paragraphs(san_report)\n","\n","    # --- failsafe: if sanitization nuked too much, fall back to raw ---\n","    if len(final_report) < 120:   # tweak if needed\n","        final_report = enforce_two_paragraphs(raw_report)\n","\n","    # factual guardrail; if it fails, try one retry and prefer longer passing candidate\n","    ok = (contains_literal(final_report, stats.get(\"winner\",\"\")) and\n","          contains_literal(final_report, format_margin(stats)) and\n","          contains_literal(final_report, stats.get(\"venue\",\"\")))\n","    if not ok:\n","        with torch.no_grad():\n","            out_ids2 = model.generate(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                repetition_penalty=1.05,\n","                no_repeat_ngram_size=4,\n","                min_new_tokens=170,\n","                max_new_tokens=250,\n","                pad_token_id=tok.pad_token_id,\n","                use_cache=True,\n","                eos_token_id=None,\n","                logits_processor=logits_processors,\n","                stopping_criteria=make_stop_criteria_after_min(tok, \"<END>\", prompt_len, 170),\n","            )\n","        cont_ids2 = out_ids2[0, prompt_len:]\n","        raw_out2 = tok.decode(cont_ids2, skip_special_tokens=True).split(\"<END>\")[0].strip()\n","        raw_out2 = clean_headings(raw_out2)\n","        raw_report2 = req_open + (\"\\n\\n\" if req_open else \"\") + raw_out2\n","        san2 = sanitize_numbers(sanitize_names(raw_report2, whitelist), stats)\n","        cand = enforce_two_paragraphs(san2)\n","\n","        def passes(txt):\n","            return (contains_literal(txt, stats.get(\"winner\",\"\")) and\n","                    contains_literal(txt, format_margin(stats)) and\n","                    contains_literal(txt, stats.get(\"venue\",\"\")))\n","        if passes(cand) and len(cand) > len(final_report):\n","            final_report = cand\n","\n","    # save RAW / SANITIZED / FINAL to Drive\n","    if save:\n","        out_dir = os.path.join(SAVE_DIR_RUN, \"reports_gen\")\n","        os.makedirs(out_dir, exist_ok=True)\n","        base = os.path.join(out_dir, f\"match_{mid}\")\n","        with open(base + \"_RAW.txt\", \"w\", encoding=\"utf-8\") as f: f.write(raw_report)\n","        with open(base + \"_SANITIZED.txt\", \"w\", encoding=\"utf-8\") as f: f.write(san_report)\n","        with open(base + \".txt\", \"w\", encoding=\"utf-8\") as f: f.write(final_report)\n","        print(\"Saved:\", base + \".txt\")\n","\n","    return final_report\n","\n","# Run once (prints full final report)\n","test_mid = match_ids[0]\n","final_txt = infer_for_match(test_mid, max_chunks=3, save=True)  # 3 chunks → faster & tighter\n","print(final_txt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtsKHwLcwFqG","executionInfo":{"status":"ok","timestamp":1755378464721,"user_tz":-120,"elapsed":26814,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"3010662f-426b-4a0a-fdef-bc9b85cf9fcc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: /content/drive/MyDrive/model_weights_tokens_files/mistral7b_fp16_20250816-200400/reports_gen/match_1422119.txt\n","Chennai Super Kings defeated Royal Challengers Bengaluru by 6 wickets at MA Chidambaram Stadium, Chepauk, Chennai.\n","\n","In the high-scoring encounter at the MA Chidambaran Stadium, Chennaii Super Kings (CSK) emerged victorious against Royal Challengers Bengluru (RCB) by 6 wickets. RCB, after winning the toss, opted to bat first and posted a competitive total of 174 runs, losing 6 wickers in their 20 overs. Anuj Rawat top-scored for RCB with 48 runs off 26 balls, while KD Karthick contributed 38 runs. For CSK, Mustafizur Rehman and C Green took 4 and 2 wickets respectively.\n","\n","Chasing a modest target, CSK got off to a steady start, losing their first wicket in the 5th over. However, Ruturai Gaikwade and Rachin Ravindra provided the impetus with a 100-run partnership for the second wicket. Gaikawade scored 64 runs off 42 balls, while Ravindra remained unbeaten on 84 runs off just 44 deliveries. The win took CSK' s winning streak against RCB to 7\n"]}]},{"cell_type":"code","source":["# Cell 10 — inference (no pipeline), with delayed stop + logits constraints + whitelist sanitizers + typo fixes + debug saves\n","\n","import os, re, json, torch\n","from collections import Counter\n","from transformers import (\n","    StoppingCriteria, StoppingCriteriaList, LogitsProcessorList,\n","    NoBadWordsLogitsProcessor, InfNanRemoveLogitsProcessor\n",")\n","\n","# ---- optional fuzzy matching (improves name correction) ----\n","try:\n","    from rapidfuzz import process as fuzz_process, fuzz\n","    HAVE_FUZZ = True\n","except Exception:\n","    HAVE_FUZZ = False\n","\n","# ---------- base helpers ----------\n","def loser_of(stats):\n","    t1, t2, w = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\"), stats.get(\"winner\",\"\")\n","    if not w: return \"\"\n","    return t2 if w == t1 else t1\n","\n","def format_margin(stats):\n","    margin = stats.get(\"result_margin\")\n","    res_txt = (stats.get(\"result\") or \"\").lower()\n","    if margin is None: return \"\"\n","    try:\n","        n = int(margin)\n","        unit = \"wickets\" if \"wicket\" in res_txt else (\"runs\" if \"run\" in res_txt else \"\")\n","        return f\"{n} {unit}\".strip()\n","    except Exception:\n","        return str(margin)\n","\n","def opening_sentence(stats):\n","    w = stats.get(\"winner\",\"\")\n","    l = loser_of(stats)\n","    m = format_margin(stats)\n","    v = stats.get(\"venue\",\"\")\n","    if not (w and l and m and v): return \"\"\n","    return f\"{w} defeated {l} by {m} at {v}.\"\n","\n","def contains_literal(text, value):\n","    if not value: return True\n","    t = re.sub(r\"\\s+\", \" \", text).lower()\n","    v = re.sub(r\"\\s+\", \" \", str(value)).lower()\n","    return v in t\n","\n","def clean_headings(txt: str) -> str:\n","    # strip any headings that slip through\n","    txt = re.sub(r\"(?m)^\\s*(P\\d:|Paragraph\\s*\\d:|Pargraph\\s*\\d:)\\s*\", \"\", txt)\n","    txt = re.sub(r\"\\s+\\.\", \".\", txt)\n","    txt = re.sub(r\"\\s+,\", \",\", txt)\n","    return txt.strip()\n","\n","# ---------- whitelist + sanitizers ----------\n","NAME_RX = re.compile(r\"[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\")\n","\n","_GENERIC_BAD = {\n","    \"Chennai\",\"Super\",\"Kings\",\"Royal\",\"Challengers\",\"Bengaluru\",\"Bangalore\",\n","    \"Stadium\",\"Chepauk\",\"Match\",\"Overs\",\"Runs\",\"Wickets\",\"Powerplay\",\n","    \"Review\",\"DRS\",\"Paragraph\",\"Pargraph\",\"Section\",\"Heading\",\"Outline\"\n","}\n","\n","def extract_names_from_text(txt: str):\n","    cands = [m.group(0).strip() for m in NAME_RX.finditer(txt)]\n","    return [c for c in cands if c not in _GENERIC_BAD and len(c) <= 30]\n","\n","def build_player_whitelist(stats, chunks):\n","    allowed = set()\n","    for key in (\"top_batters\",\"top_bowlers\",\"players\",\"batting_card\",\"bowling_card\"):\n","        val = stats.get(key)\n","        if isinstance(val, list):\n","            for item in val:\n","                if isinstance(item, dict):\n","                    for k in (\"name\",\"player\",\"batter\",\"bowler\"):\n","                        if k in item and item[k]:\n","                            allowed.add(str(item[k]).strip())\n","                elif isinstance(item, str):\n","                    allowed.add(item.strip())\n","    # names appearing in commentary multiple times\n","    counter = Counter()\n","    for c in chunks:\n","        for nm in extract_names_from_text(c):\n","            counter[nm] += 1\n","    for nm, cnt in counter.items():\n","        if cnt >= 2:\n","            allowed.add(nm)\n","    return sorted({re.sub(r\"\\s+\", \" \", a).strip() for a in allowed})\n","\n","def fuzzy_fix_name(name, whitelist):\n","    if not HAVE_FUZZ or not whitelist:\n","        return name\n","    cand, score, _ = fuzz_process.extractOne(name, whitelist, scorer=fuzz.WRatio)\n","    return cand if score >= 88 else name\n","\n","# quick deterministic typo fixes seen in outputs\n","def fix_common_typos(txt: str) -> str:\n","    replacements = {\n","        \"Chennaii\": \"Chennai\",\n","        \"Bengluru\": \"Bengaluru\",\n","        \"MA Chidambaran\": \"MA Chidambaram\",\n","        \"Chidambaran\": \"Chidambaram\",\n","        \"Gaikwade\": \"Gaikwad\",\n","        \"Gaikawade\": \"Gaikwad\",\n","        \"Ruturai\": \"Ruturaj\",\n","        \"Karthick\": \"Karthik\",    # ensure this matches your scorecard spelling\n","        \"Rehman\": \"Rahman\",\n","        \"wickers\": \"wickets\",\n","        \"CSK' s\": \"CSK's\",\n","    }\n","    for bad, good in replacements.items():\n","        txt = re.sub(rf\"\\b{re.escape(bad)}\\b\", good, txt)\n","    return txt\n","\n","def sanitize_names(text, whitelist, drop_action_lines=False):\n","    \"\"\"\n","    Replace OOV names with 'Player' instead of dropping the whole line.\n","    Set drop_action_lines=True to restore old behavior (drop lines).\n","    \"\"\"\n","    if not whitelist:\n","        return text\n","    wl_lower = {w.lower(): w for w in whitelist}\n","\n","    def is_action_line(s: str) -> bool:\n","        return re.search(\n","            r\"\\b(dismissed|bowled|caught|lbw|stumped|scored|hit|took|figures|overs|partnership)\\b\",\n","            s.lower()\n","        ) is not None\n","\n","    lines = []\n","    for line in text.splitlines():\n","        fixed = line\n","\n","        # 1) correct known names (exact or fuzzy)\n","        for n in set(extract_names_from_text(line)):\n","            rep = wl_lower.get(n.lower())\n","            if rep is None and HAVE_FUZZ:\n","                rep = fuzzy_fix_name(n, whitelist)\n","                if rep.lower() not in wl_lower:\n","                    rep = None\n","            if rep and rep != n:\n","                fixed = re.sub(rf\"\\b{re.escape(n)}\\b\", rep, fixed)\n","\n","        # 2) handle any remaining OOV names\n","        oov = [n for n in extract_names_from_text(fixed) if n.lower() not in wl_lower]\n","        if oov and is_action_line(fixed):\n","            if drop_action_lines:\n","                continue\n","            for n in oov:\n","                fixed = re.sub(rf\"\\b{re.escape(n)}\\b\", \"Player\", fixed)\n","\n","        lines.append(fixed)\n","\n","    out = \"\\n\".join(lines).strip()\n","    out = re.sub(r\"\\s+\\.\", \".\", out)\n","    out = re.sub(r\"\\s+,\", \",\", out)\n","    return out\n","\n","def valid_over_notation(text):\n","    # Normalize X.Y overs so that Y∈{0..5}\n","    return re.sub(r\"(\\b\\d+)\\.(\\d{1,2})\\s*overs\", lambda m: f\"{m.group(1)}.{min(int(m.group(2)),5)} overs\", text)\n","\n","def sanitize_numbers(text, stats):\n","    # Bowler max overs T20 = 4\n","    txt = re.sub(r\"(\\b4)\\.(\\d)\\s*overs\", r\"\\1 overs\", text)\n","    # De-fluff\n","    txt = re.sub(r\"\\brespectable total\\b\", \"a total\", txt, flags=re.I)\n","    res = (stats.get(\"result\") or \"\").lower()\n","    if \"wicket\" in res:\n","        txt = re.sub(r\"\\bfell short\\b.*?(\\.|\\n)\", \". \", txt, flags=re.I)\n","    return valid_over_notation(txt)\n","\n","def enforce_two_paragraphs(text):\n","    paras = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n","    return \"\\n\\n\".join(paras[:3])  # opener + 2\n","\n","# ---------- generation controls (delayed stop) ----------\n","class StopOnTokensAfterMin(StoppingCriteria):\n","    def __init__(self, stop_ids, prompt_len, min_new_tokens):\n","        super().__init__()\n","        self.stop_ids = stop_ids\n","        self.prompt_len = prompt_len\n","        self.min_new_tokens = min_new_tokens\n","    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n","        gen_len = input_ids.shape[-1] - self.prompt_len\n","        if gen_len < self.min_new_tokens:\n","            return False\n","        S = self.stop_ids.shape[-1]\n","        if input_ids.shape[-1] < S:\n","            return False\n","        tail = input_ids[0, -S:]\n","        return torch.equal(tail.cpu(), self.stop_ids.cpu())\n","\n","def make_stop_criteria_after_min(tokenizer, stop_str: str, prompt_len: int, min_new_tokens: int):\n","    stop_ids = tokenizer(stop_str, add_special_tokens=False, return_tensors=\"pt\").input_ids[0]\n","    return StoppingCriteriaList([StopOnTokensAfterMin(stop_ids, prompt_len, min_new_tokens)])\n","\n","def build_bad_words_ids(tokenizer, words_or_phrases):\n","    ids = []\n","    for w in words_or_phrases:\n","        toks = tokenizer(w, add_special_tokens=False).input_ids\n","        if toks: ids.append(toks)\n","    return ids\n","\n","def contradiction_phrases(stats):\n","    res = (stats.get(\"result\") or \"\").lower()\n","    if \"wicket\" in res:\n","        return [\"fell short\", \"fell just short\", \"could not chase\", \"defended the total\",\n","                \"won by runs\", \"victory by runs\", \"ran out of overs\"]\n","    if \"run\" in res:\n","        return [\"won by wickets\", \"victory by wickets\", \"chased down comfortably\",\n","                \"reached the target\", \"got over the line in the chase\"]\n","    return []\n","\n","# ---------- main inference ----------\n","def infer_for_match(mid, max_chunks=3, save=True):\n","    chunks = pick_chunks(commentary[mid][\"commentary_chunks\"], max_chunks)\n","    stats  = compact_stats(scorecards[mid][\"stats\"])\n","    req_open = opening_sentence(stats) or \"\"\n","    whitelist = build_player_whitelist(stats, chunks)\n","\n","    # Prompt: concise, two paragraphs, no headings/filler, finish with <END>, with allowed names hint\n","    allowed_names = whitelist\n","    prompt = (\n","        \"You are a cricket expert. Continue the match summary in exactly two short paragraphs (no headings).\\n\"\n","        \"The REQUIRED opening sentence has already been written. Do NOT repeat it.\\n\\n\"\n","        \"Rules:\\n\"\n","        \"- Maximum 6 sentences total across both paragraphs.\\n\"\n","        \"- Use only facts from SCORECARD and COMMENTARY EXCERPTS. Never invent.\\n\"\n","        \"- Use ONLY these player names if you mention players; if unsure, use generic phrases like 'the opener' or 'the seamer':\\n\"\n","        f\"  ALLOWED NAMES: {', '.join(allowed_names) if allowed_names else '—'}\\n\"\n","        \"- Keep it fan-friendly and factual. Do not include season-wide or streak claims unless present in SCORECARD.\\n\"\n","        \"- Do not write headings or labels.\\n\"\n","        \"- End with <END>.\\n\\n\"\n","        f\"SCORECARD:\\n{json.dumps(stats, ensure_ascii=False)}\\n\\n\"\n","        \"COMMENTARY EXCERPTS:\\n\" + \"\\n\".join(chunks) + \"\\n\\n\"\n","        \"Continue with the two paragraphs only.\"\n","    )\n","\n","    enc = tok(prompt, return_tensors=\"pt\")\n","    input_ids = enc.input_ids.to(\"cuda\")\n","    attention_mask = enc.attention_mask.to(\"cuda\")\n","\n","    # ban headings/labels & obvious contradiction phrases & common misspellings\n","    banned = [\n","        \"P1:\", \"P2:\", \"P3:\", \"Paragraph 1:\", \"Paragraph 2:\", \"Paragraph 3:\",\n","        \"Pargraph 1:\", \"Pargraph 2:\", \"Section\", \"Heading\", \"Outline:\",\n","        # season/streak hallucinations\n","        \"winning streak\", \"now won all their matches\", \"perfect record this season\",\n","        # common misspellings seen\n","        \"Chennaii\", \"Bengluru\", \"Gaikwade\", \"Gaikawade\", \"Karthick\", \"Rehman\", \"wickers\",\n","    ]\n","    for p in contradiction_phrases(stats):\n","        banned.append(p)\n","\n","    bad_words_ids = build_bad_words_ids(tok, banned)\n","    logits_processors = LogitsProcessorList([\n","        InfNanRemoveLogitsProcessor(),\n","        NoBadWordsLogitsProcessor(bad_words_ids=bad_words_ids, eos_token_id=tok.eos_token_id),\n","    ])\n","\n","    MIN_NEW = 190  # increased to avoid short outputs\n","    stop_criteria = make_stop_criteria_after_min(tok, \"<END>\", prompt_len=input_ids.shape[-1], min_new_tokens=MIN_NEW)\n","\n","    gen_kwargs = dict(\n","        do_sample=False,\n","        repetition_penalty=1.02,\n","        no_repeat_ngram_size=4,\n","        min_new_tokens=MIN_NEW,\n","        max_new_tokens=300,   # more room for two paragraphs\n","        pad_token_id=tok.pad_token_id,\n","        use_cache=True,\n","        eos_token_id=None,              # don't stop on EOS\n","        logits_processor=logits_processors,\n","        stopping_criteria=stop_criteria,\n","    )\n","\n","    with torch.no_grad():\n","        out_ids = model.generate(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            **gen_kwargs,\n","        )\n","\n","    # remove prompt (no echo) and cut at <END>\n","    prompt_len = input_ids.shape[-1]\n","    cont_ids = out_ids[0, prompt_len:]\n","    raw_out = tok.decode(cont_ids, skip_special_tokens=True)\n","    raw_out = raw_out.split(\"<END>\")[0].strip()\n","    raw_out = clean_headings(raw_out)\n","\n","    # --- assemble full report (raw) + typo fixes ---\n","    raw_report = req_open + (\"\\n\\n\" if req_open else \"\") + raw_out\n","    raw_report = fix_common_typos(raw_report)\n","\n","    # --- sanitize with whitelist & numbers ---\n","    san_report = sanitize_names(raw_report, whitelist)\n","    san_report = sanitize_numbers(san_report, stats)\n","    # if sanitization removed too much, keep raw\n","    if len(san_report.split()) < 40:\n","        san_report = raw_report\n","\n","    final_report = enforce_two_paragraphs(san_report)\n","\n","    # factual guardrail; if it fails, try one retry and prefer longer passing candidate\n","    ok = (contains_literal(final_report, stats.get(\"winner\",\"\")) and\n","          contains_literal(final_report, format_margin(stats)) and\n","          contains_literal(final_report, stats.get(\"venue\",\"\")))\n","    if not ok:\n","        with torch.no_grad():\n","            out_ids2 = model.generate(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                repetition_penalty=1.05,\n","                no_repeat_ngram_size=4,\n","                min_new_tokens=180,\n","                max_new_tokens=290,\n","                pad_token_id=tok.pad_token_id,\n","                use_cache=True,\n","                eos_token_id=None,\n","                logits_processor=logits_processors,\n","                stopping_criteria=make_stop_criteria_after_min(tok, \"<END>\", prompt_len, 180),\n","            )\n","        cont_ids2 = out_ids2[0, prompt_len:]\n","        raw_out2 = tok.decode(cont_ids2, skip_special_tokens=True).split(\"<END>\")[0].strip()\n","        raw_out2 = clean_headings(raw_out2)\n","        raw_report2 = req_open + (\"\\n\\n\" if req_open else \"\") + raw_out2\n","        raw_report2 = fix_common_typos(raw_report2)\n","        san2 = sanitize_numbers(sanitize_names(raw_report2, whitelist), stats)\n","        if len(san2.split()) < 40:\n","            san2 = raw_report2\n","        cand = enforce_two_paragraphs(san2)\n","\n","        def passes(txt):\n","            return (contains_literal(txt, stats.get(\"winner\",\"\")) and\n","                    contains_literal(txt, format_margin(stats)) and\n","                    contains_literal(txt, stats.get(\"venue\",\"\")))\n","        if passes(cand) and len(cand) > len(final_report):\n","            final_report = cand\n","\n","    # save RAW / SANITIZED / FINAL to Drive\n","    if save:\n","        out_dir = os.path.join(SAVE_DIR_RUN, \"reports_gen\")\n","        os.makedirs(out_dir, exist_ok=True)\n","        base = os.path.join(out_dir, f\"match_{mid}\")\n","        with open(base + \"_RAW.txt\", \"w\", encoding=\"utf-8\") as f: f.write(raw_report)\n","        with open(base + \"_SANITIZED.txt\", \"w\", encoding=\"utf-8\") as f: f.write(san_report)\n","        with open(base + \".txt\", \"w\", encoding=\"utf-8\") as f: f.write(final_report)\n","        print(\"Saved:\", base + \".txt\")\n","\n","    return final_report\n","\n","# Run once (prints full final report)\n","test_mid = match_ids[0]\n","final_txt = infer_for_match(test_mid, max_chunks=3, save=True)  # 3 chunks → tighter & faster\n","print(final_txt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b2mzudxywi04","executionInfo":{"status":"ok","timestamp":1755379317482,"user_tz":-120,"elapsed":31099,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"74491c9d-365c-4695-cef1-01944b300315"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: /content/drive/MyDrive/model_weights_tokens_files/mistral7b_fp16_20250816-200400/reports_gen/match_1422119.txt\n","Chennai Super Kings defeated Royal Challengers Bengaluru by 6 wickets at MA Chidambaram Stadium, Chepauk, Chennai.\n","\n","Anuj RawAT and KD KARTHik opened for Player (RCB), with Rawat scoring 48 off 26 balls, including 5 fours and 1 six. KD Player contributed 38 off 32 balls, hitting 4 fours. Player, their efforts were not enough to propel RCB to a competitive total, as they were restricted to 174/6 in their 20 overs.\n","\n","For Chennai SuperKings (CSK), MustafizUR Mustafizur Rahman was the standout bowler, taking 4 wickets for 30 runs in 4 overs. C Green also chipped in with 2 wickets for just 27 runs in 3.3 overs. Player response, CSK reached their target with 6 wickets and 4 balls to spare, thanks to a 44-ball 67 from Rachin GAikwad and a quickfire 19-ball 33 from Faf du PlessIS. Du Plessis' innings included 3 fours and a six, while GaikwAD hit 7 fours and as many sixes. The win was a significant one for CSK, as it marked their 10th consecutive victory against RCB in the Player (IPL). Player you for your response.\n"]}]},{"cell_type":"markdown","source":["### **Cell 11: Quick Fact check & ROUGE-L**"],"metadata":{"id":"R9hoI5jov_b1"}},{"cell_type":"code","source":["import re\n","\n","def contains_literal(text, value):\n","    if not value: return True\n","    t = re.sub(r\"\\s+\", \" \", text).lower()\n","    v = re.sub(r\"\\s+\", \" \", str(value)).lower()\n","    return v in t\n","\n","def compute_loser(stats):\n","    t1,t2,w = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\"), stats.get(\"winner\",\"\")\n","    if not w: return \"\"\n","    return t2 if w==t1 else t1\n","\n","def lcs_len(a,b):\n","    n,m=len(a),len(b)\n","    dp=[[0]*(m+1) for _ in range(n+1)]\n","    for i in range(1,n+1):\n","        ai=a[i-1]\n","        for j in range(1,m+1):\n","            dp[i][j]=dp[i-1][j-1]+1 if ai==b[j-1] else max(dp[i-1][j],dp[i][j-1])\n","    return dp[n][m]\n","\n","def rouge_l(pred, ref):\n","    p, r = pred.split(), ref.split()\n","    if not p or not r: return {\"r\":0.0,\"p\":0.0,\"f\":0.0}\n","    lcs = lcs_len(p,r); rec=lcs/len(r); prec=lcs/len(p)\n","    f = 0.0 if rec+prec==0 else (2*prec*rec)/(prec+rec+1e-12)\n","    return {\"r\":rec,\"p\":prec,\"f\":f}\n","\n","gold = reports[test_mid][\"report_text\"]\n","stats = compact_stats(scorecards[test_mid][\"stats\"])\n","\n","facts_ok = {\n","    \"winner\": contains_literal(gen_text, stats.get(\"winner\",\"\")),\n","    \"margin\": contains_literal(gen_text, str(stats.get(\"result_margin\",\"\"))),\n","    \"venue\":  contains_literal(gen_text, stats.get(\"venue\",\"\")),\n","    \"toss\":   (contains_literal(gen_text, stats.get(\"toss_winner\",\"\")) and contains_literal(gen_text, stats.get(\"toss_decision\",\"\"))),\n","    \"loser\":  contains_literal(gen_text, compute_loser(stats)) if compute_loser(stats) else True\n","}\n","facts_ok[\"all_pass\"] = all(facts_ok.values())\n","\n","rl = rouge_l(gen_text, gold)\n","print(\"FACTS:\", facts_ok)\n","print(\"ROUGE-L:\", rl)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRoMoyZswSfV","executionInfo":{"status":"ok","timestamp":1755373824036,"user_tz":-120,"elapsed":56,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"c5cb17cc-0cc9-4132-f840-d56d38c3afb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["FACTS: {'winner': True, 'margin': False, 'venue': True, 'toss': True, 'loser': True, 'all_pass': False}\n","ROUGE-L: {'r': 0.12686567164179105, 'p': 0.1574074074074074, 'f': 0.14049586776810083}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"mY8Bch63yeX5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZeBpfvCx-k3l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IYflxPNR-ksC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **NEW RUN to keep the output fresh**"],"metadata":{"id":"n7ZuPMdncHzv"}},{"cell_type":"code","source":["# Cell 1 — GPU & runtime check\n","# Purpose: verify GPU, seed, environment\n","\n","import os, random, torch, platform, numpy as np\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","\n","!nvidia-smi || true\n","print(\"Python:\", platform.python_version())\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"GPU:\", torch.cuda.get_device_name(0))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ReQWhSzIcOke","executionInfo":{"status":"ok","timestamp":1755618802932,"user_tz":-120,"elapsed":4596,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"05d3ef5e-bcaa-4b0d-c385-b62b7a7dd843"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Aug 19 15:53:22 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n","| N/A   61C    P8             14W /   72W |       0MiB /  23034MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n","Python: 3.11.13\n","CUDA available: True\n","GPU: NVIDIA L4\n"]}]},{"cell_type":"code","source":["# Cell 2 — Install minimal, stable deps (no bitsandbytes)\n","# Purpose: keep stack simple & stable for FP16 LoRA on A100\n","\n","!pip -q install --no-cache-dir \\\n","  \"transformers==4.43.3\" \\\n","  \"peft==0.12.0\" \\\n","  \"accelerate==0.31.0\" \\\n","  \"datasets==2.20.0\" \\\n","  \"sentence-transformers==3.0.1\" \\\n","  \"rapidfuzz==3.9.7\" \\\n","  \"trafilatura==1.8.0\""],"metadata":{"id":"VMpxTDyTchwk","executionInfo":{"status":"ok","timestamp":1755618882178,"user_tz":-120,"elapsed":76103,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f2877f95-346a-41b3-fe2d-370b3a8a1091"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m337.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m347.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m367.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m345.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m202.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m352.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m337.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m387.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m350.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m347.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m331.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m362.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m347.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m349.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m353.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m247.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m307.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m313.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m237.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m341.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m339.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# Cell 3 — Mount Drive & set paths, model, and held-out test match\n","# Purpose: path config and versioned save dir\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import time, os\n","\n","# -------- data paths --------\n","DATA_DIR = \"/content/drive/MyDrive/Thesis_data\"   # <- change if needed\n","COMMENTARY_FP = f\"{DATA_DIR}/commentary.jsonl\"\n","SCORECARDS_FP = f\"{DATA_DIR}/scorecards.jsonl\"\n","REPORTS_FP    = f\"{DATA_DIR}/reports.jsonl\"\n","\n","# -------- choose model --------\n","# Default (recommended on L4/T4/A100 when A100 not available):\n","MODEL_ID = \"meta-llama/Meta-Llama-3.1-3B-Instruct\"\n","\n","# If you later want to try Qwen 7B, just uncomment the next line:\n","# MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n","\n","# Pretty name for save dir based on model choice\n","model_tag = (\n","    \"llama31_3b_fp16\"\n","    if \"llama\" in MODEL_ID.lower()\n","    else (\"qwen25_7b_fp16\" if \"qwen\" in MODEL_ID.lower() else \"model_fp16\")\n",")\n","\n","STAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n","SAVE_ROOT = \"/content/drive/MyDrive/model_weights_tokens_files\"\n","SAVE_DIR_RUN = f\"{SAVE_ROOT}/{model_tag}_{STAMP}\"\n","os.makedirs(SAVE_DIR_RUN, exist_ok=True)\n","\n","# Held-out test match\n","TEST_ID  = \"1422119\"   # RCB vs CSK (change if needed)\n","\n","print(\"DATA_DIR:\", DATA_DIR)\n","print(\"SAVE_DIR_RUN:\", SAVE_DIR_RUN)\n","print(\"MODEL_ID:\", MODEL_ID)\n","print(\"TEST_ID:\", TEST_ID)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRhvyXVXDCnY","executionInfo":{"status":"ok","timestamp":1755619037828,"user_tz":-120,"elapsed":22299,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"386d4c72-b849-4ec3-99cf-193a51621d56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","DATA_DIR: /content/drive/MyDrive/Thesis_data\n","SAVE_DIR_RUN: /content/drive/MyDrive/model_weights_tokens_files/llama31_3b_fp16_20250819-155716\n","MODEL_ID: meta-llama/Meta-Llama-3.1-3B-Instruct\n","TEST_ID: 1422119\n"]}]},{"cell_type":"code","source":["# Cell 3 — Mount Drive & set paths, model, and held-out test match\n","# Purpose: path config and versioned save dir\n","\n","# FOR MISTRAL AI ONLY\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import time, os\n","DATA_DIR = \"/content/drive/MyDrive/Thesis_data\"   # <- change if needed\n","COMMENTARY_FP = f\"{DATA_DIR}/commentary.jsonl\"\n","SCORECARDS_FP = f\"{DATA_DIR}/scorecards.jsonl\"\n","REPORTS_FP    = f\"{DATA_DIR}/reports.jsonl\"\n","\n","STAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n","SAVE_ROOT = \"/content/drive/MyDrive/model_weights_tokens_files\"\n","SAVE_DIR_RUN = f\"{SAVE_ROOT}/mistral7b_fp16_{STAMP}\"\n","os.makedirs(SAVE_DIR_RUN, exist_ok=True)\n","\n","MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","TEST_ID  = \"1422119\"   # RCB vs CSK (change if needed)\n","\n","print(\"DATA_DIR:\", DATA_DIR)\n","print(\"SAVE_DIR_RUN:\", SAVE_DIR_RUN)\n","print(\"MODEL_ID:\", MODEL_ID)\n","print(\"TEST_ID:\", TEST_ID)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x6obxyoW7YdW","executionInfo":{"status":"ok","timestamp":1755620064127,"user_tz":-120,"elapsed":1417,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"dcb0c00e-6ce7-41f8-97af-9f0da3299e5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","DATA_DIR: /content/drive/MyDrive/Thesis_data\n","SAVE_DIR_RUN: /content/drive/MyDrive/model_weights_tokens_files/mistral7b_fp16_20250819-161423\n","MODEL_ID: meta-llama/Meta-Llama-3.1-8B-Instruct\n","TEST_ID: 1422119\n"]}]},{"cell_type":"code","source":["# Cell 4 — Load JSONL & compute splits (no train_manifest)\n","# Purpose: read the three files and hold out TEST_ID\n","\n","import json\n","\n","def load_jsonl(path):\n","    rows=[]\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            if line.strip(): rows.append(json.loads(line))\n","    return rows\n","\n","def by_id(rows, key=\"match_id\"):\n","    return {str(r[key]): r for r in rows if key in r}\n","\n","commentary_rows = load_jsonl(COMMENTARY_FP)\n","scorecard_rows  = load_jsonl(SCORECARDS_FP)\n","report_rows     = load_jsonl(REPORTS_FP)\n","\n","commentary_by_id = by_id(commentary_rows)\n","scorecards_by_id = by_id(scorecard_rows)\n","reports_by_id    = by_id(report_rows)\n","\n","all_ids = sorted(set(commentary_by_id) & set(scorecards_by_id))\n","gold_ids = sorted(set(all_ids) & set(reports_by_id))\n","\n","print(\"Matches with commentary+scorecard:\", len(all_ids))\n","print(\"Gold reports:\", len(gold_ids), \"e.g.\", gold_ids[:10])\n","assert TEST_ID in all_ids, \"TEST_ID missing from your data files.\"\n","\n","train_ids = [m for m in all_ids if m != TEST_ID]\n","print(\"Train size (held-out test):\", len(train_ids))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5f1d44ul75d3","executionInfo":{"status":"ok","timestamp":1755620065925,"user_tz":-120,"elapsed":30,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"837144f8-d78f-4537-db3c-71001966ff87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Matches with commentary+scorecard: 71\n","Gold reports: 4 e.g. ['1422119', '1422120', '1422121', '1422122']\n","Train size (held-out test): 70\n"]}]},{"cell_type":"code","source":["# Cell 5 — Core helpers: compact stats, chunk picking, opener, templates\n","# Purpose: consistent fields, minimal templated targets for weak supervision\n","\n","import re\n","from collections import Counter\n","\n","def get(obj, key, default=None):\n","    try: return obj.get(key, default)\n","    except Exception: return default\n","\n","def compact_stats(stats):\n","    return {\n","        \"team1\": get(stats,\"team1\",\"\"), \"team2\": get(stats,\"team2\",\"\"),\n","        \"winner\": get(stats,\"winner\",\"\"), \"result\": get(stats,\"result\",\"\"),\n","        \"result_margin\": get(stats,\"result_margin\",\"\"), \"venue\": get(stats,\"venue\",\"\"),\n","        \"toss_winner\": get(stats,\"toss_winner\",\"\"), \"toss_decision\": get(stats,\"toss_decision\",\"\"),\n","        \"date\": get(stats,\"date\",\"\"),\n","        \"top_batters\": get(stats,\"top_batters\",[]), \"top_bowlers\": get(stats,\"top_bowlers\",[]),\n","        \"batting_card_t1\": get(stats,\"batting_card_t1\",[]), \"batting_card_t2\": get(stats,\"batting_card_t2\",[]),\n","        \"bowling_card_t1\": get(stats,\"bowling_card_t1\",[]), \"bowling_card_t2\": get(stats,\"bowling_card_t2\",[]),\n","        \"players_t1\": get(stats,\"players_t1\",[]), \"players_t2\": get(stats,\"players_t2\",[]),\n","    }\n","\n","def pick_chunks(chunks, max_chunks=4):\n","    if not chunks: return []\n","    if len(chunks) <= max_chunks: return chunks\n","    idxs = [0, len(chunks)//2, -1]\n","    s = set(i for i in idxs if 0<=i<len(chunks))\n","    while len(s) < max_chunks: s.add(len(s))\n","    return [chunks[i] for i in sorted(list(s))[:max_chunks]]\n","\n","def loser_of(stats):\n","    t1,t2,w = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\"), stats.get(\"winner\",\"\")\n","    return t2 if w == t1 else t1\n","\n","def format_margin(stats):\n","    margin = stats.get(\"result_margin\")\n","    res = (stats.get(\"result\") or \"\").lower()\n","    if margin in (None,\"\"): return \"\"\n","    try:\n","        n = int(margin)\n","        unit = \"wickets\" if \"wicket\" in res else (\"runs\" if \"run\" in res else \"\")\n","        return f\"{n} {unit}\".strip()\n","    except Exception:\n","        return str(margin)\n","\n","def opening_sentence(stats):\n","    w, l, m, v = stats.get(\"winner\",\"\"), loser_of(stats), format_margin(stats), stats.get(\"venue\",\"\")\n","    if not (w and l and m and v): return \"\"\n","    return f\"{w} defeated {l} by {m} at {v}.\"\n","\n","def extract_name_list(stats):\n","    names=[]\n","    for li in (\"top_batters\",\"top_bowlers\",\"players_t1\",\"players_t2\"):\n","        for it in stats.get(li,[]) or []:\n","            if isinstance(it,dict):\n","                for k in (\"name\",\"player\",\"batter\",\"bowler\"):\n","                    if k in it and it[k]: names.append(str(it[k]))\n","            elif isinstance(it,str):\n","                names.append(it)\n","    seen=set(); out=[]\n","    for n in names:\n","        if n not in seen: seen.add(n); out.append(n)\n","    return out\n","\n","def template_two_paragraphs(stats):\n","    opener = opening_sentence(stats)\n","    p1 = \"The match turned on a handful of overs where wickets and boundaries flipped momentum.\"\n","    sb_name=\"\"; sb_runs=\"\"\n","    for b in stats.get(\"top_batters\",[]):\n","        if isinstance(b,dict) and b.get(\"batter\"):\n","            sb_name=b[\"batter\"]; sb_runs=b.get(\"runs\",\"\"); break\n","    bw_name=\"\"; bw_wkts=\"\"\n","    for w in stats.get(\"top_bowlers\",[]):\n","        if isinstance(w,dict) and w.get(\"bowler\"):\n","            bw_name=w[\"bowler\"]; bw_wkts=w.get(\"wkts\",\"\"); break\n","    tail=[]\n","    if sb_name and sb_runs!=\"\": tail.append(f\"{sb_name} led the scoring with {sb_runs}.\")\n","    if bw_name and bw_wkts!=\"\": tail.append(f\"{bw_name} stood out with {bw_wkts} wickets.\")\n","    if stats.get(\"toss_winner\"): tail.append(f\"{stats['toss_winner']} won the toss and chose to {stats.get('toss_decision','bat')}.\")\n","    tail.append(\"The result shapes the next steps in the tournament.\")\n","    p2=\" \".join(tail)\n","    return (opener + \"\\n\\n\" + p1 + \"\\n\\n\" + p2).strip()\n","\n","print(\"Core helpers ready.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DyTSu43U8A5U","executionInfo":{"status":"ok","timestamp":1755620067541,"user_tz":-120,"elapsed":18,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"83115f37-de58-4a99-f25c-68949c260430"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Core helpers ready.\n"]}]},{"cell_type":"code","source":["# Cell 6 — Use existing web_summaries.jsonl if present; else build from CSV\n","# Purpose: prefer your curated summaries; only construct from URLs if missing/empty.\n","\n","import os, csv, json, re, trafilatura\n","\n","DATA_DIR = \"/content/drive/MyDrive/Thesis_data\"\n","URL_CSV = f\"{DATA_DIR}/web_reports_urls.csv\"\n","SILVER_JSONL = f\"{DATA_DIR}/web_summaries.jsonl\"\n","TEST_ID = \"1422119\"  # held-out; do not include in silver to avoid leakage\n","\n","def count_jsonl(path):\n","    if not os.path.exists(path): return 0\n","    n = 0\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            if line.strip(): n += 1\n","    return n\n","\n","silver_count = count_jsonl(SILVER_JSONL)\n","\n","if silver_count > 0:\n","    print(f\"Found existing web_summaries.jsonl with {silver_count} items → using it as-is.\")\n","else:\n","    print(\"No existing web_summaries.jsonl found (or empty). Attempting to build from web_reports_urls.csv ...\")\n","\n","    BAD_PATTERNS = [\n","        r\"Live\\s+blog\", r\"as it happened\", r\"minute-by-minute\", r\"ball-by-ball\",\n","        r\"subscribe\", r\"sign up\", r\"advertisement\", r\"cookie\", r\"privacy policy\",\n","    ]\n","\n","    def load_pairs(csv_path):\n","        rows=[]\n","        if not os.path.exists(csv_path):\n","            print(\"No web_reports_urls.csv found — skipping web silver build.\")\n","            return rows\n","        with open(csv_path,\"r\",encoding=\"utf-8\") as f:\n","            rd = csv.DictReader(f)\n","            for r in rd:\n","                mid = str(r.get(\"match_id\",\"\")).strip()\n","                url = (r.get(\"url\") or \"\").strip()\n","                if mid and url: rows.append((mid,url))\n","        return rows\n","\n","    def fetch_clean(url):\n","        try:\n","            raw = trafilatura.fetch_url(url, no_ssl=True)\n","            if not raw: return \"\"\n","            txt = trafilatura.extract(raw, include_comments=False, include_tables=False, favor_precision=True) or \"\"\n","            return re.sub(r\"\\s+\",\" \", txt).strip()\n","        except Exception:\n","            return \"\"\n","\n","    def basic_cleanup(txt):\n","        for pat in BAD_PATTERNS:\n","            txt = re.sub(pat, \"\", txt, flags=re.IGNORECASE)\n","        words = txt.split()\n","        if len(words) > 280: txt = \" \".join(words[:280])\n","        return txt\n","\n","    def to_two_paras(txt):\n","        sents = re.split(r'(?<=[.!?])\\s+', txt)\n","        p1 = \" \".join(sents[:3]).strip()\n","        p2 = \" \".join(sents[3:6]).strip() or \" \".join(sents[2:5]).strip()\n","        if not p2: p2 = \"The result reflects the decisive passages of play.\"\n","        out = (p1 + \"\\n\\n\" + p2).strip()\n","        return \" \".join(out.split()[:230])\n","\n","    pairs = load_pairs(URL_CSV)\n","    n = 0\n","    if pairs:\n","        with open(SILVER_JSONL, \"w\", encoding=\"utf-8\") as out:\n","            for mid, url in pairs:\n","                if mid == TEST_ID:  # avoid leakage into eval\n","                    continue\n","                text = fetch_clean(url)\n","                if not text or len(text.split()) < 60:  # too short/noisy\n","                    continue\n","                text = basic_cleanup(text)\n","                shaped = to_two_paras(text)\n","                if len(shaped.split()) < 60:\n","                    continue\n","                rec = {\"match_id\": mid, \"report_text\": shaped, \"source_url\": url, \"source_type\": \"silver_web\"}\n","                out.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n","                n += 1\n","        print(f\"Wrote {n} items -> {SILVER_JSONL}\")\n","    else:\n","        print(\"No URL CSV to build from. Proceeding without web silver.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yLUWUvg8HMR","executionInfo":{"status":"ok","timestamp":1755620068855,"user_tz":-120,"elapsed":21,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"2374de3a-9664-4fd9-ec18-cd0ea79b9b8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing web_summaries.jsonl with 11 items → using it as-is.\n"]}]},{"cell_type":"code","source":["# A) Time file I/O from Drive vs local\n","import time, shutil, os, json, itertools, pathlib\n","\n","DATA_DIR = \"/content/drive/MyDrive/thesis_data\"\n","LOCAL = \"/content/thesis_tmp\"\n","os.makedirs(LOCAL, exist_ok=True)\n","\n","t0=time.time()\n","for fn in [\"commentary.jsonl\",\"scorecards.jsonl\",\"reports.jsonl\",\"web_summaries.jsonl\"]:\n","    src=f\"{DATA_DIR}/{fn}\"\n","    if os.path.exists(src):\n","        shutil.copy2(src, LOCAL)\n","print(\"Copy -> /content/ :\", round(time.time()-t0,2),\"s\")\n","\n","# Quick size + first lines\n","for fn in os.listdir(LOCAL):\n","    p=f\"{LOCAL}/{fn}\"\n","    print(fn, \"| MB:\", os.path.getsize(p)/1e6)\n","    with open(p,'r',encoding='utf-8') as f:\n","        print(\"head:\", next(itertools.islice(f,0,1)).strip()[:160])"],"metadata":{"id":"iCyb1Yh5KeLc","executionInfo":{"status":"ok","timestamp":1755620070250,"user_tz":-120,"elapsed":4,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"49cd5e16-ebd7-4ac0-eb16-e76624ca6049","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Copy -> /content/ : 0.0 s\n"]}]},{"cell_type":"code","source":["# B) Re-run just the *build training list* step with timers and progress\n","from time import time\n","import json, re\n","from collections import Counter\n","\n","def load_jsonl(path):\n","    rows=[]\n","    if not os.path.exists(path): return rows\n","    with open(path,\"r\",encoding=\"utf-8\") as f:\n","        for line in f:\n","            if line.strip(): rows.append(json.loads(line))\n","    return rows\n","\n","COMMENTARY_FP = f\"{LOCAL}/commentary.jsonl\"\n","SCORECARDS_FP = f\"{LOCAL}/scorecards.jsonl\"\n","REPORTS_FP    = f\"{LOCAL}/reports.jsonl\"\n","SILVER_FP     = f\"{LOCAL}/web_summaries.jsonl\"\n","\n","commentary_rows = load_jsonl(COMMENTARY_FP)\n","scorecard_rows  = load_jsonl(SCORECARDS_FP)\n","report_rows     = load_jsonl(REPORTS_FP)\n","silver_rows     = load_jsonl(SILVER_FP)\n","\n","def by_id(rows, key=\"match_id\"): return {str(r[key]): r for r in rows if key in r}\n","commentary_by_id = by_id(commentary_rows)\n","scorecards_by_id = by_id(scorecard_rows)\n","reports_by_id    = by_id(report_rows)\n","silver_by_id     = by_id(silver_rows)\n","\n","all_ids = sorted(set(commentary_by_id) & set(scorecards_by_id))\n","TEST_ID = \"1422119\"\n","train_ids = [m for m in all_ids if m != TEST_ID]\n","\n","print(\"counts | comm:\",len(commentary_rows),\"score:\",len(scorecard_rows),\"gold:\",len(report_rows),\"silver:\",len(silver_rows))\n","print(\"train_ids:\", len(train_ids))\n","\n","# Minimal helpers (adapt from your notebook)\n","def get(o,k,d=None):\n","    try: return o.get(k,d)\n","    except: return d\n","\n","def compact_stats(stats):\n","    return {\n","        \"team1\":get(stats,\"team1\",\"\"),\"team2\":get(stats,\"team2\",\"\"),\n","        \"winner\":get(stats,\"winner\",\"\"),\"result\":get(stats,\"result\",\"\"),\n","        \"result_margin\":get(stats,\"result_margin\",\"\"),\"venue\":get(stats,\"venue\",\"\"),\n","        \"toss_winner\":get(stats,\"toss_winner\",\"\"),\"toss_decision\":get(stats,\"toss_decision\",\"\"),\n","        \"top_batters\":get(stats,\"top_batters\",[]),\"top_bowlers\":get(stats,\"top_bowlers\",[]),\n","        \"players_t1\":get(stats,\"players_t1\",[]),\"players_t2\":get(stats,\"players_t2\",[]),\n","        \"batting_card_t1\":get(stats,\"batting_card_t1\",[]),\"batting_card_t2\":get(stats,\"batting_card_t2\",[]),\n","        \"bowling_card_t1\":get(stats,\"bowling_card_t1\",[]),\"bowling_card_t2\":get(stats,\"bowling_card_t2\",[]),\n","    }\n","\n","def pick_chunks(chunks, max_chunks=4):\n","    if not chunks: return []\n","    return chunks[:max_chunks]\n","\n","def loser_of(s):\n","    t1,t2,w=s.get(\"team1\",\"\"),s.get(\"team2\",\"\"),s.get(\"winner\",\"\")\n","    return t2 if w==t1 else t1\n","\n","def format_margin(s):\n","    m=s.get(\"result_margin\"); r=(s.get(\"result\") or \"\").lower()\n","    if m in (None,\"\"): return \"\"\n","    try:\n","        n=int(m); unit=\"wickets\" if \"wicket\" in r else (\"runs\" if \"run\" in r else \"\")\n","        return f\"{n} {unit}\".strip()\n","    except: return str(m)\n","\n","def opening_sentence(s):\n","    w,l,m,v=s.get(\"winner\",\"\"),loser_of(s),format_margin(s),s.get(\"venue\",\"\")\n","    return f\"{w} defeated {l} by {m} at {v}.\" if (w and l and m and v) else \"\"\n","\n","def extract_name_list(s):\n","    names=[]\n","    for li in (\"top_batters\",\"top_bowlers\",\"players_t1\",\"players_t2\"):\n","        for it in s.get(li,[]) or []:\n","            if isinstance(it,dict):\n","                for k in (\"name\",\"player\",\"batter\",\"bowler\"):\n","                    if it.get(k): names.append(str(it[k]))\n","            elif isinstance(it,str):\n","                names.append(it)\n","    out=[]; seen=set()\n","    for n in names:\n","        if n not in seen: seen.add(n); out.append(n)\n","    return out\n","\n","def template_two_paragraphs(s):\n","    op=opening_sentence(s)\n","    p1=\"The match turned on a handful of overs where wickets and boundaries flipped momentum.\"\n","    p2=\"The result shapes the next steps in the tournament.\"\n","    return (op+\"\\n\\n\"+p1+\"\\n\\n\"+p2).strip()\n","\n","def normalize_target_to_two_paras(text, opener):\n","    body=text.strip().replace(opener,\"\").strip() if opener else text.strip()\n","    sents=re.split(r'(?<=[.!?])\\s+', body)\n","    p1=\" \".join(sents[:3]).strip()\n","    p2=\" \".join(sents[3:6]).strip() or \" \".join(sents[2:5]).strip() or \"The result underscores the decisive passages of play.\"\n","    return (opener+\"\\n\\n\"+p1+\"\\n\\n\"+p2).strip()\n","\n","def build_training_example(mid, max_chunks=6):\n","    com=commentary_by_id[mid]; sc=scorecards_by_id[mid]; stats=compact_stats(sc[\"stats\"])\n","    chunks=pick_chunks(com.get(\"commentary_chunks\",[]), max_chunks=max_chunks)\n","    opener=opening_sentence(stats)\n","    if mid in reports_by_id:\n","        target=normalize_target_to_two_paras(reports_by_id[mid][\"report_text\"], opener); label=\"gold\"\n","    elif mid in silver_by_id:\n","        target=normalize_target_to_two_paras(silver_by_id[mid][\"report_text\"], opener); label=\"silver\"\n","    else:\n","        target=template_two_paragraphs(stats); label=\"template\"\n","    allowed=\", \".join(extract_name_list(stats)) or \"—\"\n","    prompt = (\n","        \"You are a cricket expert. Continue the match summary in exactly two short paragraphs (no headings).\\n\"\n","        \"The REQUIRED opening sentence has already been written. Do NOT repeat it.\\n\\n\"\n","        \"Rules:\\n- Maximum 6 sentences total across both paragraphs.\\n\"\n","        \"- Use only facts from SCORECARD and COMMENTARY EXCERPTS. Never invent.\\n\"\n","        f\"- ALLOWED NAMES: {allowed}\\n- End with <END>.\\n\\n\"\n","        f\"SCORECARD:\\n{json.dumps(stats, ensure_ascii=False)}\\n\\n\"\n","        \"COMMENTARY EXCERPTS:\\n\" + \"\\n\".join(chunks) + \"\\n\\n\"\n","        \"Continue with the two paragraphs only.\"\n","    )\n","    return {\"match_id\": mid, \"text\": prompt + \"\\n\" + target + \" <END>\", \"label_type\": label}\n","\n","t0=time()\n","train_set=[]\n","for i,mid in enumerate(train_ids,1):\n","    train_set.append(build_training_example(mid))\n","    if i%10==0:\n","        print(f\"built {i}/{len(train_ids)} examples...\")\n","print(\"Done. secs:\", round(time()-t0,2))\n","from collections import Counter\n","print(\"labels:\", Counter([t[\"label_type\"] for t in train_set]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cMQiuyZE9Z_A","executionInfo":{"status":"ok","timestamp":1755620071719,"user_tz":-120,"elapsed":24,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"fe41eaa2-657a-425b-92c8-766d70f78cfe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["counts | comm: 0 score: 0 gold: 0 silver: 0\n","train_ids: 0\n","Done. secs: 0.0\n","labels: Counter()\n"]}]},{"cell_type":"code","source":["# Cell 7 — Build training set = GOLD + (optional) SILVER + TEMPLATES (exclude TEST_ID)\n","# Purpose: mitigate overfitting; create uniform two-paragraph targets; guarantee non-empty output if data exist\n","\n","import os, json, re\n","from collections import Counter\n","\n","# ---- paths (use local copy for speed; change to your DATA_DIR if needed) ----\n","DATA_DIR = \"/content/drive/MyDrive/Thesis_data\"\n","COMMENTARY_FP = f\"{DATA_DIR}/commentary.jsonl\"\n","SCORECARDS_FP = f\"{DATA_DIR}/scorecards.jsonl\"\n","REPORTS_FP    = f\"{DATA_DIR}/reports.jsonl\"\n","SILVER_FP     = f\"{DATA_DIR}/web_summaries.jsonl\"\n","TEST_ID       = \"1422119\"\n","\n","def load_jsonl(path):\n","    rows = []\n","    if not os.path.exists(path): return rows\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            line = line.strip()\n","            if line:\n","                try:\n","                    rows.append(json.loads(line))\n","                except Exception:\n","                    pass\n","    return rows\n","\n","def by_id(rows, key=\"match_id\"):\n","    out = {}\n","    for r in rows:\n","        if key in r:\n","            out[str(r[key])] = r\n","    return out\n","\n","# ---- load all datasets ----\n","commentary_rows = load_jsonl(COMMENTARY_FP)\n","scorecard_rows  = load_jsonl(SCORECARDS_FP)\n","report_rows     = load_jsonl(REPORTS_FP)\n","silver_rows     = load_jsonl(SILVER_FP)\n","\n","commentary_by_id = by_id(commentary_rows)\n","scorecards_by_id = by_id(scorecard_rows)\n","reports_by_id    = by_id(report_rows)\n","silver_by_id     = by_id(silver_rows)\n","\n","# ---- compute train ids: intersection minus TEST_ID ----\n","all_ids = sorted(set(commentary_by_id) & set(scorecards_by_id))\n","train_ids = [m for m in all_ids if m != TEST_ID]\n","\n","print(f\"Loaded - commentary:{len(commentary_rows)} scorecards:{len(scorecard_rows)} \"\n","      f\"gold:{len(report_rows)} silver:{len(silver_rows)}\")\n","print(f\"Intersected match_ids: {len(all_ids)} | train_ids (excl TEST): {len(train_ids)}\")\n","\n","# ---- helpers reused from earlier cells (safe, minimal) ----\n","def get(o,k,d=None):\n","    try: return o.get(k,d)\n","    except: return d\n","\n","def compact_stats(stats):\n","    return {\n","        \"team1\":get(stats,\"team1\",\"\"), \"team2\":get(stats,\"team2\",\"\"),\n","        \"winner\":get(stats,\"winner\",\"\"), \"result\":get(stats,\"result\",\"\"),\n","        \"result_margin\":get(stats,\"result_margin\",\"\"), \"venue\":get(stats,\"venue\",\"\"),\n","        \"toss_winner\":get(stats,\"toss_winner\",\"\"), \"toss_decision\":get(stats,\"toss_decision\",\"\"),\n","        \"top_batters\":get(stats,\"top_batters\",[]), \"top_bowlers\":get(stats,\"top_bowlers\",[]),\n","        \"players_t1\":get(stats,\"players_t1\",[]), \"players_t2\":get(stats,\"players_t2\",[]),\n","        \"batting_card_t1\":get(stats,\"batting_card_t1\",[]), \"batting_card_t2\":get(stats,\"batting_card_t2\",[]),\n","        \"bowling_card_t1\":get(stats,\"bowling_card_t1\",[]), \"bowling_card_t2\":get(stats,\"bowling_card_t2\",[]),\n","    }\n","\n","def pick_chunks(chunks, max_chunks=6):\n","    if not isinstance(chunks, list) or len(chunks) == 0:\n","        return []\n","    return chunks[:max_chunks]\n","\n","def loser_of(stats):\n","    t1, t2, w = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\"), stats.get(\"winner\",\"\")\n","    return t2 if w == t1 else t1\n","\n","def format_margin(stats):\n","    margin = stats.get(\"result_margin\")\n","    res_txt = (stats.get(\"result\") or \"\").lower()\n","    if margin in (None, \"\"): return \"\"\n","    try:\n","        n = int(margin)\n","        unit = \"wickets\" if \"wicket\" in res_txt else (\"runs\" if \"run\" in res_txt else \"\")\n","        return f\"{n} {unit}\".strip()\n","    except Exception:\n","        return str(margin)\n","\n","def opening_sentence(stats):\n","    w = stats.get(\"winner\",\"\")\n","    l = loser_of(stats)\n","    m = format_margin(stats)\n","    v = stats.get(\"venue\",\"\")\n","    return f\"{w} defeated {l} by {m} at {v}.\" if (w and l and m and v) else \"\"\n","\n","def extract_name_list(stats):\n","    names = []\n","    for li in (\"top_batters\",\"top_bowlers\",\"players_t1\",\"players_t2\"):\n","        for it in stats.get(li,[]) or []:\n","            if isinstance(it, dict):\n","                for k in (\"name\",\"player\",\"batter\",\"bowler\"):\n","                    if it.get(k): names.append(str(it[k]))\n","            elif isinstance(it, str):\n","                names.append(it)\n","    # uniq, preserve order\n","    out, seen = [], set()\n","    for n in names:\n","        if n not in seen:\n","            seen.add(n); out.append(n)\n","    return out\n","\n","def template_two_paragraphs(stats):\n","    op = opening_sentence(stats)\n","    p1 = \"The match hinged on a few key overs where wickets and boundaries flipped momentum.\"\n","    p2 = \"Standout spells and partnerships shaped the result in the closing stages.\"\n","    return (op + \"\\n\\n\" + p1 + \"\\n\\n\" + p2).strip()\n","\n","def normalize_target_to_two_paras(text, opener):\n","    body = (text or \"\").strip()\n","    if opener and opener in body:\n","        body = body.replace(opener, \"\").strip()\n","    sents = re.split(r'(?<=[.!?])\\s+', body)\n","    p1 = \" \".join(sents[:3]).strip()\n","    p2 = \" \".join(sents[3:6]).strip() or \" \".join(sents[2:5]).strip()\n","    if not p2:\n","        p2 = \"The result underscores the decisive passages of play.\"\n","    trimmed = (opener + \"\\n\\n\" + p1 + \"\\n\\n\" + p2).strip()\n","    return \" \".join(trimmed.split()[:230])\n","\n","# ---- main builder (uses preloaded silver_by_id once; no per-item re-read) ----\n","def build_training_example(mid, max_chunks=6):\n","    com = commentary_by_id.get(mid); sc = scorecards_by_id.get(mid)\n","    if not com or not sc:\n","        return None\n","    stats = compact_stats(sc.get(\"stats\", {}))\n","    chunks = pick_chunks(com.get(\"commentary_chunks\", []), max_chunks=max_chunks)\n","    opener = opening_sentence(stats)\n","\n","    source_type = \"template\"\n","    target = template_two_paragraphs(stats)\n","\n","    if mid in reports_by_id:\n","        target = normalize_target_to_two_paras(get(reports_by_id[mid],\"report_text\",\"\"), opener)\n","        source_type = \"gold\"\n","    elif mid in silver_by_id:\n","        target = normalize_target_to_two_paras(get(silver_by_id[mid],\"report_text\",\"\"), opener)\n","        source_type = \"silver\"\n","\n","    allowed_names = \", \".join(extract_name_list(stats)) or \"—\"\n","    prompt = (\n","        \"You are a cricket expert. Continue the match summary in exactly two short paragraphs (no headings).\\n\"\n","        \"The REQUIRED opening sentence has already been written. Do NOT repeat it.\\n\\n\"\n","        \"Rules:\\n\"\n","        \"- Maximum 6 sentences total across both paragraphs.\\n\"\n","        \"- Use only facts from SCORECARD and COMMENTARY EXCERPTS. Never invent.\\n\"\n","        \"- Use ONLY these player names if you mention players; if unsure, use generic phrases like 'the opener' or 'the seamer':\\n\"\n","        f\"  ALLOWED NAMES: {allowed_names}\\n\"\n","        \"- Do not include season-wide or streak claims unless present in SCORECARD.\\n\"\n","        \"- Do not write headings or labels.\\n\"\n","        \"- End with <END>.\\n\\n\"\n","        f\"SCORECARD:\\n{json.dumps(stats, ensure_ascii=False)}\\n\\n\"\n","        \"COMMENTARY EXCERPTS:\\n\" + \"\\n\".join(chunks) + \"\\n\\n\"\n","        \"Continue with the two paragraphs only.\"\n","    )\n","    return {\"match_id\": mid, \"text\": prompt + \"\\n\" + target + \" <END>\", \"label_type\": source_type}\n","\n","train_set = []\n","for mid in train_ids:\n","    ex = build_training_example(mid)\n","    if ex and isinstance(ex.get(\"text\"), str) and len(ex[\"text\"]) > 50:\n","        train_set.append(ex)\n","\n","print(\"Train items:\", len(train_set), \"| labels:\", Counter([t[\"label_type\"] for t in train_set]))\n","if len(train_set) == 0:\n","    print(\"⚠️ train_set is empty. Check that commentary.jsonl and scorecards.jsonl share the same match_id values,\")\n","    print(\"   and that you copied them to\", DATA_DIR, \"or updated the paths above.\")\n","else:\n","    # show an example id and a short preview so you can confirm structure\n","    sample = train_set[0]\n","    print(\"Sample match_id:\", sample[\"match_id\"])\n","    print(\"Prompt preview:\", sample[\"text\"][:220].replace(\"\\n\",\" \") + \" ...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HE6cT8pyECD0","executionInfo":{"status":"ok","timestamp":1755620072880,"user_tz":-120,"elapsed":29,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"80260f46-abe2-4c1f-aa18-6557ec3a2035"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded - commentary:71 scorecards:71 gold:4 silver:11\n","Intersected match_ids: 71 | train_ids (excl TEST): 70\n","Train items: 70 | labels: Counter({'template': 56, 'silver': 11, 'gold': 3})\n","Sample match_id: 1422120\n","Prompt preview: You are a cricket expert. Continue the match summary in exactly two short paragraphs (no headings). The REQUIRED opening sentence has already been written. Do NOT repeat it.  Rules: - Maximum 6 sentences total across bot ...\n"]}]},{"cell_type":"code","source":["# Cell 8a — Align Triton/BnB to Torch 2.6 (cu124) and free VRAM\n","\n","import gc, os, torch\n","for name in [\"trainer\",\"model\",\"base_model\",\"base\",\"gen_pipe\",\"tok\"]:\n","    if name in globals():\n","        try: del globals()[name]\n","        except: pass\n","gc.collect()\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()\n","\n","print(\"torch:\", torch.__version__, \"| cuda:\", torch.version.cuda)\n","\n","# Install exact matches for torch 2.6.0+cu124\n","!pip -q install --no-cache-dir --upgrade \\\n","  \"triton==3.2.0\" \\\n","  \"bitsandbytes==0.45.0\" \\\n","  \"accelerate>=0.34.2\" \\\n","  \"transformers>=4.43.3\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gVXjnQjKlj9","executionInfo":{"status":"ok","timestamp":1755620080270,"user_tz":-120,"elapsed":4956,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"282e0aee-6080-4a14-d33a-9afcb79ce8d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch: 2.6.0+cu124 | cuda: 12.4\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["# Cell 7.9 — Clean out stray bitsandbytes, ensure Triton OK\n","# Purpose: avoid accidental bnb import on FP16 path\n","\n","import os, sys, subprocess, importlib\n","\n","# 1) Uninstall bitsandbytes if present\n","try:\n","    import bitsandbytes  # noqa\n","    print(\"bitsandbytes is installed -> removing...\")\n","    subprocess.check_call([\"pip\", \"uninstall\", \"-y\", \"bitsandbytes\"])\n","except Exception:\n","    print(\"bitsandbytes not installed. OK.\")\n","\n","# 2) Make sure Triton matches Torch (Colab usually has torch==2.6.0+cu124 -> triton==3.2.0)\n","import torch\n","torch_ver = torch.__version__\n","print(\"torch:\", torch_ver, \"| cuda:\", torch.version.cuda)\n","want_triton = \"3.2.0\" if torch_ver.startswith(\"2.6\") else (\"2.3.1\" if torch_ver.startswith(\"2.3\") else None)\n","\n","if want_triton:\n","    try:\n","        import triton\n","        print(\"triton:\", triton.__version__)\n","        if triton.__version__ != want_triton:\n","            print(f\"Installing triton=={want_triton} ...\")\n","            subprocess.check_call([\"pip\", \"install\", \"--no-cache-dir\", f\"triton=={want_triton}\"])\n","    except Exception:\n","        if want_triton:\n","            print(f\"Installing triton=={want_triton} ...\")\n","            subprocess.check_call([\"pip\", \"install\", \"--no-cache-dir\", f\"triton=={want_triton}\"])\n","\n","# 3) Sanity: ensure bnb can’t be imported anymore\n","if \"bitsandbytes\" in sys.modules:\n","    del sys.modules[\"bitsandbytes\"]\n","try:\n","    import bitsandbytes  # noqa\n","    raise RuntimeError(\"bitsandbytes still importable; re-run this cell once more.\")\n","except Exception:\n","    print(\"✅ bitsandbytes not importable. Proceed with Cell 8.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LqeeUjdlZgz3","executionInfo":{"status":"ok","timestamp":1755620084224,"user_tz":-120,"elapsed":52,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"700bf2d7-f047-4225-868d-fcc85d7d0148"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bitsandbytes not installed. OK.\n","torch: 2.6.0+cu124 | cuda: 12.4\n","triton: 3.2.0\n","✅ bitsandbytes not importable. Proceed with Cell 8.\n"]}]},{"cell_type":"code","source":["# Cell 7.9 — Hard clean bitsandbytes (safe path checks)\n","\n","import sys, importlib, subprocess, os, shutil, site, glob\n","\n","def _silent_run(cmd):\n","    try:\n","        subprocess.check_call(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.STDOUT)\n","    except subprocess.CalledProcessError:\n","        pass\n","\n","print(\"Uninstall bitsandbytes via pip (if present)...\")\n","_silent_run([\"pip\", \"uninstall\", \"-y\", \"bitsandbytes\"])\n","\n","# Candidate site-packages roots in Colab\n","candidates = set()\n","for p in [\n","    getattr(site, \"getusersitepackages\", lambda: None)(),\n","    *(getattr(site, \"getsitepackages\", lambda: [])() or []),\n","    \"/usr/local/lib/python3.11/dist-packages\",\n","    \"/usr/local/lib/python3.10/dist-packages\",\n","    \"/usr/local/lib/python3.9/dist-packages\",\n","]:\n","    if p and os.path.isdir(p):\n","        candidates.add(p)\n","\n","removed = []\n","for sp in candidates:\n","    # remove folders and dist-info matching bitsandbytes*\n","    for pat in [\"bitsandbytes*\", \"bitsandbytes_cuda*\"]:\n","        for path in glob.glob(os.path.join(sp, pat)):\n","            try:\n","                if os.path.isdir(path):\n","                    shutil.rmtree(path, ignore_errors=True)\n","                elif os.path.isfile(path):\n","                    os.remove(path)\n","                removed.append(path)\n","            except Exception:\n","                pass\n","\n","# Flush import cache and re-check\n","importlib.invalidate_caches()\n","if \"bitsandbytes\" in sys.modules:\n","    del sys.modules[\"bitsandbytes\"]\n","\n","print(\"Removed paths:\", removed if removed else \"(none found)\")\n","try:\n","    import bitsandbytes  # noqa\n","    raise RuntimeError(\"bitsandbytes is STILL importable — run this cell once more or Restart runtime (Runtime > Restart).\")\n","except Exception:\n","    print(\"✅ bitsandbytes not importable. Proceed to Cell 8 (FP16, no-bnb).\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ih9SUpvBaC0C","executionInfo":{"status":"ok","timestamp":1755620086473,"user_tz":-120,"elapsed":851,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"942f6d98-2509-4010-b833-ccc3ad7067b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Uninstall bitsandbytes via pip (if present)...\n","Removed paths: (none found)\n","✅ bitsandbytes not importable. Proceed to Cell 8 (FP16, no-bnb).\n"]}]},{"cell_type":"code","source":["# Cell 7.10 — swap PEFT to a version that doesn't import bitsandbytes at import time\n","\n","!pip -q uninstall -y peft || true\n","!pip -q install --no-cache-dir peft==0.9.0\n","\n","import importlib, sys\n","if \"peft\" in sys.modules:\n","    import peft as _peft_old\n","    importlib.reload(_peft_old)\n","print(\"✅ PEFT pinned to 0.9.0\")"],"metadata":{"id":"nc8qpEW7eJNl","executionInfo":{"status":"ok","timestamp":1755620091769,"user_tz":-120,"elapsed":3618,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"ec7f002e-7070-4093-c981-8b0e17520b32","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/190.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h✅ PEFT pinned to 0.9.0\n"]}]},{"cell_type":"code","source":["# Cell 8 — Load base model (BF16) + attach LoRA  (REPLACEMENT)\n","import os, torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import LoraConfig, get_peft_model\n","\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","\n","# MODEL_ID is set in Cell 3\n","tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_ID,\n","    torch_dtype=torch.bfloat16,   # ← BF16 for training on A100/L4\n","    attn_implementation=\"sdpa\",\n","    device_map=\"auto\",\n",")\n","base_model.gradient_checkpointing_enable()\n","base_model.config.use_cache = False\n","\n","lora_cfg = LoraConfig(\n","    r=8, lora_alpha=16, lora_dropout=0.05,\n","    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n","    bias=\"none\", task_type=\"CAUSAL_LM\",\n",")\n","\n","model = get_peft_model(base_model, lora_cfg)\n","model.print_trainable_parameters()\n","print(\"✅ BF16 + LoRA ready.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":690},"id":"v6-kf2_DGUww","executionInfo":{"status":"error","timestamp":1755620093891,"user_tz":-120,"elapsed":665,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"f5b3675c-4668-49a6-d011-d4426d34ff3e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n403 Client Error. (Request ID: Root=1-68a4a2fd-4c2f98407797bbed2b184040;f6b72bcb-502d-4017-a311-cca72cf81615)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct to ask for access.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1547\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    425\u001b[0m             )\n\u001b[0;32m--> 426\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGatedRepoError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-68a4a2fd-4c2f98407797bbed2b184040;f6b72bcb-502d-4017-a311-cca72cf81615)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct to ask for access.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1223478832.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# MODEL_ID is set in Cell 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m   1070\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m                     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    709\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_raise_exceptions_for_gated_repo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    544\u001b[0m                 \u001b[0;34m\"You are trying to access a gated repo.\\nMake sure to have access to it at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0;34mf\"https://huggingface.co/{path_or_repo_id}.\\n{str(e)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n403 Client Error. (Request ID: Root=1-68a4a2fd-4c2f98407797bbed2b184040;f6b72bcb-502d-4017-a311-cca72cf81615)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct to ask for access."]}]},{"cell_type":"code","source":["# Cell 8 — Load base model (FP16, no bitsandbytes) + attach LoRA\n","# Purpose: avoid any bnb dependency; force PEFT to use plain torch layers.\n","\n","import os, torch, importlib\n","\n","# 1) Make absolutely sure PEFT won't import bnb adapters\n","os.environ[\"PEFT_DISABLE_BNB_ADAPTERS\"] = \"1\"   # <-- key line (must be set BEFORE importing peft)\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import peft\n","importlib.reload(peft)  # ensure the env var takes effect if peft was imported earlier\n","from peft import LoraConfig, get_peft_model\n","\n","# 2) Torch runtime tweaks\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n","torch.backends.cuda.matmul.allow_tf32 = True\n","torch.backends.cudnn.allow_tf32 = True\n","\n","# 3) Model + tokenizer\n","MODEL_ID = \"meta-llama/Meta-Llama-3.1-3B-Instruct\"\n","tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n","if tok.pad_token is None:\n","    tok.pad_token = tok.eos_token\n","\n","# 4) Load base in FP16, let Accelerate place layers\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_ID,\n","    torch_dtype=torch.float16,\n","    attn_implementation=\"sdpa\",\n","    device_map=\"auto\",\n",")\n","base_model.gradient_checkpointing_enable()\n","base_model.config.use_cache = False\n","\n","# 5) Plain LoRA on attention + MLP projections (no bnb layers)\n","lora_cfg = LoraConfig(\n","    r=8, lora_alpha=16, lora_dropout=0.05,\n","    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n","    bias=\"none\", task_type=\"CAUSAL_LM\",\n",")\n","\n","model = get_peft_model(base_model, lora_cfg)\n","model.print_trainable_parameters()\n","print(\"✅ FP16 + LoRA ready (no bitsandbytes).\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":639},"id":"vmGzH6g0bryq","executionInfo":{"status":"error","timestamp":1755619736928,"user_tz":-120,"elapsed":246,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"1f1db408-a000-4de4-c1f2-cd7f289eeb89"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"meta-llama/Meta-Llama-3.1-3B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-3B-Instruct/resolve/main/tokenizer_config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1659\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1546\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1547\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1462\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m429\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    458\u001b[0m             )\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-68a4a198-5521645601e259976bdfb4d1;16b46b40-1f54-4f03-a877-a62afab5b603)\n\nRepository Not Found for url: https://huggingface.co/meta-llama/Meta-Llama-3.1-3B-Instruct/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-390558594.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# 3) Model + tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mMODEL_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"meta-llama/Meta-Llama-3.1-3B-Instruct\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0mcommit_hash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m     resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# We cannot recover from them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: meta-llama/Meta-Llama-3.1-3B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"]}]},{"cell_type":"code","source":["# Cell 9 — Train with a plain Torch Dataset (bypasses HF Datasets/Numpy/Arrow)\n","# Purpose: avoid NumPy 2.x copy issues; keep everything in torch tensors.\n","\n","import os, time, json, torch\n","from torch.utils.data import Dataset as TorchDataset\n","from transformers import TrainingArguments, Trainer, default_data_collator\n","\n","assert \"train_set\" in globals() and len(train_set) > 0, \"Run Cell 7 first.\"\n","assert \"tok\" in globals() and \"model\" in globals(), \"Run Cell 8 first.\"\n","\n","MAX_LEN = 1536  # if VRAM is tight, try 1280 or 1024\n","\n","def encode_text(example_text: str):\n","    enc = tok(\n","        example_text,\n","        truncation=True,\n","        max_length=MAX_LEN,\n","        padding=\"max_length\",\n","        return_tensors=\"pt\",\n","    )\n","    # labels = input_ids for causal LM\n","    enc[\"labels\"] = enc[\"input_ids\"].clone()\n","    # squeeze to 1D tensors\n","    return {k: v.squeeze(0) for k, v in enc.items()}\n","\n","# Pre-tokenize once (fast & saves memory fragmentation during training)\n","encoded = [encode_text(row[\"text\"]) for row in train_set]\n","\n","class SimpleLMDataset(TorchDataset):\n","    def __init__(self, enc_list): self.data = enc_list\n","    def __len__(self): return len(self.data)\n","    def __getitem__(self, idx): return self.data[idx]\n","\n","torch_ds = SimpleLMDataset(encoded)\n","print(\"Train samples:\", len(torch_ds))\n","\n","args = TrainingArguments(\n","    output_dir=\"/content/lora_out\",\n","    learning_rate=2e-4,\n","    num_train_epochs=1,                # 2–3 for final runs\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=8,\n","    logging_steps=20,\n","    warmup_ratio=0.05,\n","    lr_scheduler_type=\"cosine\",\n","    fp16=True,                         # using FP16 weights\n","    report_to=\"none\",\n","    remove_unused_columns=False,       # important with custom torch dataset\n","    ddp_find_unused_parameters=False,\n","    max_grad_norm=0.5,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=torch_ds,\n","    data_collator=default_data_collator,   # no extra padding/shaping — we already padded\n",")\n","\n","trainer.train()\n","\n","# Save to a versioned run dir\n","SAVE_DIR = \"/content/drive/MyDrive/model_weights_tokens_files\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","ts = time.strftime(\"%Y%m%d-%H%M%S\")\n","SAVE_DIR_RUN = os.path.join(SAVE_DIR, f\"mistral7b_fp16_noshard_{ts}\")\n","os.makedirs(SAVE_DIR_RUN, exist_ok=True)\n","\n","model.save_pretrained(SAVE_DIR_RUN)\n","tok.save_pretrained(SAVE_DIR_RUN)\n","with open(os.path.join(SAVE_DIR_RUN, \"train_manifest_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n","    json.dump(\n","        {\n","            \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n","            \"train_count\": len(train_set),\n","            \"max_len\": MAX_LEN,\n","            \"notes\": \"FP16 LoRA; no sharding; torch Dataset; excludes TEST_ID.\",\n","        },\n","        f,\n","        indent=2,\n","    )\n","print(\"Saved to:\", SAVE_DIR_RUN)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"pyJht78dMnFi","executionInfo":{"status":"error","timestamp":1755619716866,"user_tz":-120,"elapsed":20065,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"cc4966f4-5bad-4f68-bd18-1514a253ef8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train samples: 70\n"]},{"output_type":"error","ename":"ValueError","evalue":"Attempting to unscale FP16 gradients.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2818422370.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Save to a versioned run dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2239\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2621\u001b[0m                                     \u001b[0mgrad_norm_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimplicit_replication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m                                 \u001b[0;32mwith\u001b[0m \u001b[0mgrad_norm_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m                                     _grad_norm = self.accelerator.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   2624\u001b[0m                                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m                                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2888\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2890\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2891\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36munscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2826\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAcceleratedOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    258\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mallow_fp16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempting to unscale FP16 gradients.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                         \u001b[0;31m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."]}]},{"cell_type":"code","source":["# Cell 10 — Inference (guarded) with name whitelist + anti-ball-by-ball + fallback if too short\n","# Purpose: avoid hallucinated ball-by-ball, keep only allowed names, and back off to role-based prose if needed.\n","\n","import os, re, json, torch\n","from collections import Counter\n","from transformers import (\n","    StoppingCriteria, StoppingCriteriaList, LogitsProcessorList,\n","    NoBadWordsLogitsProcessor, InfNanRemoveLogitsProcessor\n",")\n","\n","assert \"SAVE_DIR_RUN\" in globals(), \"Run Cell 9 first.\"\n","assert \"commentary_by_id\" in globals() and \"scorecards_by_id\" in globals(), \"Run Cell 7 first.\"\n","TEST_ID = globals().get(\"TEST_ID\", \"1422119\")\n","\n","# ---------- helpers (same as before; trimmed where safe) ----------\n","def loser_of(stats):\n","    t1, t2, w = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\"), stats.get(\"winner\",\"\")\n","    return t2 if w == t1 else t1\n","\n","def format_margin(stats):\n","    margin = stats.get(\"result_margin\"); res_txt=(stats.get(\"result\") or \"\").lower()\n","    if margin in (None,\"\"): return \"\"\n","    try:\n","        n=int(margin); unit=\"wickets\" if \"wicket\" in res_txt else (\"runs\" if \"run\" in res_txt else \"\")\n","        return f\"{n} {unit}\".strip()\n","    except: return str(margin)\n","\n","def opening_sentence(stats):\n","    w=stats.get(\"winner\",\"\"); l=loser_of(stats); m=format_margin(stats); v=stats.get(\"venue\",\"\")\n","    return f\"{w} defeated {l} by {m} at {v}.\" if (w and l and m and v) else \"\"\n","\n","def compact_stats(stats):\n","    get=lambda o,k,d=None:(o or {}).get(k,d)\n","    return {\n","        \"team1\":get(stats,\"team1\",\"\"), \"team2\":get(stats,\"team2\",\"\"),\n","        \"winner\":get(stats,\"winner\",\"\"), \"result\":get(stats,\"result\",\"\"),\n","        \"result_margin\":get(stats,\"result_margin\",\"\"), \"venue\":get(stats,\"venue\",\"\"),\n","        \"toss_winner\":get(stats,\"toss_winner\",\"\"), \"toss_decision\":get(stats,\"toss_decision\",\"\"),\n","        \"top_batters\":get(stats,\"top_batters\",[]), \"top_bowlers\":get(stats,\"top_bowlers\",[]),\n","        \"players_t1\":get(stats,\"players_t1\",[]), \"players_t2\":get(stats,\"players_t2\",[]),\n","        \"batting_card_t1\":get(stats,\"batting_card_t1\",[]), \"batting_card_t2\":get(stats,\"batting_card_t2\",[]),\n","        \"bowling_card_t1\":get(stats,\"bowling_card_t1\",[]), \"bowling_card_t2\":get(stats,\"bowling_card_t2\",[]),\n","    }\n","\n","def pick_chunks(chunks, max_chunks=4): return (chunks or [])[:max_chunks]\n","\n","def contains_literal(text, value):\n","    if not value: return True\n","    t=re.sub(r\"\\s+\",\" \",text).lower(); v=re.sub(r\"\\s+\",\" \",str(value)).lower()\n","    return v in t\n","\n","def clean_headings(txt):\n","    txt=re.sub(r\"(?m)^\\s*(P\\d:|Paragraph\\s*\\d:|Pargraph\\s*\\d:)\\s*\",\"\",txt)\n","    txt=re.sub(r\"\\s+\\.\",\".\",txt); txt=re.sub(r\"\\s+,\",\",\",txt)\n","    return txt.strip()\n","\n","# whitelist + sanitizers\n","NAME_RX = re.compile(r\"[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\")\n","_GENERIC_BAD = {\"Chennai\",\"Super\",\"Kings\",\"Royal\",\"Challengers\",\"Bengaluru\",\"Bangalore\",\n","                \"Stadium\",\"Chepauk\",\"Match\",\"Overs\",\"Runs\",\"Wickets\",\"Powerplay\",\"Review\",\n","                \"DRS\",\"Paragraph\",\"Pargraph\",\"Section\",\"Heading\",\"Outline\"}\n","\n","def extract_names_from_text(txt):\n","    cands=[m.group(0).strip() for m in NAME_RX.finditer(txt)]\n","    return [c for c in cands if c not in _GENERIC_BAD and len(c)<=30]\n","\n","def build_player_whitelist(stats, chunks):\n","    # From structured stats\n","    allowed=set()\n","    for key in (\"top_batters\",\"top_bowlers\",\"players_t1\",\"players_t2\",\n","                \"batting_card_t1\",\"batting_card_t2\",\"bowling_card_t1\",\"bowling_card_t2\"):\n","        for item in (stats or {}).get(key, []):\n","            if isinstance(item, dict):\n","                for k in (\"name\",\"player\",\"batter\",\"bowler\"):\n","                    if item.get(k): allowed.add(str(item[k]).strip())\n","            elif isinstance(item, str):\n","                allowed.add(item.strip())\n","    # Frequent names from commentary (only if appear >=3 times)\n","    freq=Counter()\n","    for c in chunks:\n","        for nm in extract_names_from_text(c): freq[nm]+=1\n","    for nm,cnt in freq.items():\n","        if cnt>=3: allowed.add(nm)\n","    return sorted({re.sub(r\"\\s+\",\" \",a).strip() for a in allowed})\n","\n","def sanitize_names(text, whitelist):\n","    if not whitelist: return text\n","    wl_lower={w.lower():w for w in whitelist}\n","    lines=[]\n","    for line in text.splitlines():\n","        fixed=line\n","        for n in set(extract_names_from_text(line)):\n","            rep=wl_lower.get(n.lower(), n)\n","            if rep!=n:\n","                fixed=re.sub(rf\"\\b{re.escape(n)}\\b\", rep, fixed)\n","        # drop lines that still contain unknown names AND claim specific actions\n","        oov=[n for n in extract_names_from_text(fixed) if n.lower() not in wl_lower]\n","        if oov and re.search(r\"\\b(dismissed|bowled|caught|lbw|stumped|run out|scored|hit|smashed|took|figures|overs|partnership)\\b\", fixed.lower()):\n","            continue\n","        lines.append(fixed)\n","    return \"\\n\".join(lines).strip()\n","\n","def valid_over_notation(text): return re.sub(r\"(\\b4)\\.(\\d)\\s*overs\", r\"\\1 overs\", text)\n","\n","def sanitize_numbers(text, stats):\n","    txt=re.sub(r\"\\brespectable total\\b\",\"a total\",text,flags=re.I)\n","    res=(stats.get(\"result\") or \"\").lower()\n","    if \"wicket\" in res:\n","        txt=re.sub(r\"\\bfell short\\b.*?(\\.|\\n)\",\". \",txt,flags=re.I)\n","    return valid_over_notation(txt)\n","\n","def enforce_two_paragraphs(text):\n","    paras=[p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n","    return \"\\n\\n\".join(paras[:3])\n","\n","# decoding controls\n","class StopOnTokens(StoppingCriteria):\n","    def __init__(self, stop_ids): self.stop_ids=stop_ids\n","    def __call__(self, input_ids, scores, **kwargs)->bool:\n","        S=self.stop_ids.shape[-1]\n","        if input_ids.shape[-1]<S: return False\n","        return torch.equal(input_ids[0,-S:].cpu(), self.stop_ids.cpu())\n","\n","def make_stop_criteria(tokenizer, stop_str):\n","    stop_ids=tokenizer(stop_str, add_special_tokens=False, return_tensors=\"pt\").input_ids[0]\n","    return StoppingCriteriaList([StopOnTokens(stop_ids)])\n","\n","def build_bad_words_ids(tokenizer, words):\n","    ids=[]\n","    for w in words:\n","        toks=tokenizer(w, add_special_tokens=False).input_ids\n","        if toks: ids.append(toks)\n","    return ids\n","\n","def contradiction_phrases(stats):\n","    res=(stats.get(\"result\") or \"\").lower()\n","    if \"wicket\" in res:  # winner chased\n","        return [\"fell short\",\"could not chase\",\"defended the total\",\"won by runs\",\"victory by runs\",\"ran out of overs\"]\n","    if \"run\" in res:     # winner defended\n","        return [\"won by wickets\",\"victory by wickets\",\"chased down comfortably\",\"reached the target\",\"got over the line in the chase\"]\n","    return []\n","\n","# extra bans to kill ball-by-ball tone\n","BALL_BY_BALL_BANS = [\n","    \"first ball\", \"second ball\", \"third ball\", \"fourth ball\", \"fifth ball\", \"sixth ball\",\n","    \"next ball\", \"the very next ball\", \"over-by-over\", \"ball-by-ball\",\n","    \"beams past\", \"attempted pull shot\", \"york length\", \"length ball that swings\",\n","]\n","\n","def build_prompt(stats, chunks, whitelist, forbid_names=False):\n","    opener = opening_sentence(stats)\n","    # Names policy\n","    if forbid_names:\n","        names_line = \"- Do not use player names; use generic roles like 'the opener' or 'the seamer'.\\n\"\n","    else:\n","        allowed_names = \", \".join(whitelist) or \"—\"\n","        names_line = (\n","            \"- Use ONLY these player names if you mention players; \"\n","            \"otherwise use roles like 'the opener' or 'the seamer':\\n\"\n","            f\"  ALLOWED NAMES: {allowed_names}\\n\"\n","        )\n","    return (\n","        \"You are a cricket expert. Continue the match summary in exactly two short paragraphs (no headings).\\n\"\n","        \"The REQUIRED opening sentence has already been written. Do NOT repeat it.\\n\\n\"\n","        \"Rules:\\n\"\n","        \"- Max 6 sentences total across both paragraphs.\\n\"\n","        \"- Use only facts from SCORECARD and COMMENTARY EXCERPTS. Never invent.\\n\"\n","        \"- Write aggregate events (key overs/partnerships) — DO NOT narrate ball-by-ball.\\n\"\n","        f\"{names_line}\"\n","        \"- Do not include season-wide or streak claims unless present in SCORECARD.\\n\"\n","        \"- End with <END>.\\n\\n\"\n","        f\"SCORECARD:\\n{json.dumps(stats, ensure_ascii=False)}\\n\\n\"\n","        \"COMMENTARY EXCERPTS:\\n\" + \"\\n\".join(chunks) + \"\\n\\n\"\n","        \"Continue with the two paragraphs only.\"\n","    ), opener\n","\n","def generate_once(prompt, min_nt=150, max_nt=230, banned_extra=None):\n","    enc = tok(prompt, return_tensors=\"pt\")\n","    input_ids = enc.input_ids.to(model.device)\n","    attention_mask = enc.attention_mask.to(model.device)\n","\n","    banned = [\"P1:\",\"P2:\",\"Paragraph 1:\",\"Paragraph 2:\",\"Section\",\"Heading\",\"Outline:\"]\n","    banned += (banned_extra or [])\n","    bad_words_ids = build_bad_words_ids(tok, banned)\n","\n","    logits_processors = LogitsProcessorList([\n","        InfNanRemoveLogitsProcessor(),\n","        NoBadWordsLogitsProcessor(bad_words_ids=bad_words_ids, eos_token_id=tok.eos_token_id),\n","    ])\n","    stop_criteria = make_stop_criteria(tok, \"<END>\")\n","\n","    gen_kwargs = dict(\n","        do_sample=False,\n","        repetition_penalty=1.02,\n","        no_repeat_ngram_size=4,\n","        min_new_tokens=min_nt,\n","        max_new_tokens=max_nt,\n","        pad_token_id=tok.pad_token_id,\n","        use_cache=True,\n","        eos_token_id=None,\n","        logits_processor=logits_processors,\n","        stopping_criteria=stop_criteria,\n","    )\n","\n","    with torch.no_grad():\n","        out_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_kwargs)\n","\n","    prompt_len = input_ids.shape[-1]\n","    cont_ids = out_ids[0, prompt_len:]\n","    raw = tok.decode(cont_ids, skip_special_tokens=True).split(\"<END>\")[0].strip()\n","    return clean_headings(raw)\n","\n","# Additional sanitizer to strip ball-by-ball invented details\n","def strip_ball_by_ball_sentences(text: str) -> str:\n","    patterns = [\n","        r\"\\bcaught at\\b\", r\"\\byork(ed|er)?\\b\", r\"\\brun out\\b\", r\"\\bbowled\\b\",\n","        r\"\\bstumped\\b\", r\"\\bslower ball\\b\", r\"\\battempted\\b\", r\"\\b(first|second|third|fourth|fifth|sixth) ball\\b\",\n","        r\"\\b18th over\\b\", r\"\\b19th over\\b\", r\"\\b20th over\\b\"\n","    ]\n","    out_sents=[]\n","    for sent in re.split(r'(?<=[.!?])\\s+', text):\n","        if any(re.search(p, sent, flags=re.I) for p in patterns):\n","            continue\n","        out_sents.append(sent)\n","    return \" \".join(out_sents).strip()\n","\n","def infer_for_match(mid: str, max_chunks=4, save=True):\n","    com = commentary_by_id[mid]; sc = scorecards_by_id[mid]\n","    stats = compact_stats(sc[\"stats\"])\n","    chunks = pick_chunks(com.get(\"commentary_chunks\", []), max_chunks=max_chunks)\n","    whitelist = build_player_whitelist(stats, chunks)\n","\n","    # Pass 1 — allow names (from whitelist), ban ball-by-ball phrases\n","    prompt1, opener = build_prompt(stats, chunks, whitelist, forbid_names=False)\n","    raw1 = generate_once(prompt1, min_nt=140, max_nt=220, banned_extra=contradiction_phrases(stats)+BALL_BY_BALL_BANS)\n","    report_raw = opener + (\"\\n\\n\" if opener else \"\") + raw1\n","\n","    # Sanitize\n","    report_san = sanitize_names(report_raw, whitelist)\n","    report_san = sanitize_numbers(report_san, stats)\n","    report_san = strip_ball_by_ball_sentences(report_san)\n","    report_san = enforce_two_paragraphs(report_san)\n","\n","    # Guardrails: ensure key literals and minimal length; else fallback Pass 2 (no names)\n","    need_fallback = False\n","    for needed in (stats.get(\"winner\",\"\"), format_margin(stats), stats.get(\"venue\",\"\")):\n","        if not contains_literal(report_san, needed):\n","            need_fallback = True\n","            break\n","    if len(report_san) < 240:  # too short after sanitization\n","        need_fallback = True\n","\n","    if need_fallback:\n","        prompt2, opener2 = build_prompt(stats, chunks, whitelist, forbid_names=True)\n","        raw2 = generate_once(prompt2, min_nt=130, max_nt=210, banned_extra=contradiction_phrases(stats)+BALL_BY_BALL_BANS)\n","        report_raw2 = opener2 + (\"\\n\\n\" if opener2 else \"\") + raw2\n","        report_san2 = sanitize_numbers(report_raw2, stats)  # no name sanitization needed (we forbade names)\n","        report_san2 = enforce_two_paragraphs(report_san2)\n","        # prefer the longer, constraint-satisfying version\n","        if len(report_san2) >= len(report_san):\n","            report_san = report_san2\n","            report_raw = report_raw2\n","\n","    # Save both RAW & SANITIZED\n","    out_dir = os.path.join(SAVE_DIR_RUN, \"reports_gen\"); os.makedirs(out_dir, exist_ok=True)\n","    with open(os.path.join(out_dir, f\"{mid}_RAW.txt\"), \"w\", encoding=\"utf-8\") as f: f.write(report_raw)\n","    with open(os.path.join(out_dir, f\"{mid}_SANITIZED.txt\"), \"w\", encoding=\"utf-8\") as f: f.write(report_san)\n","    print(\"Saved:\", os.path.join(out_dir, f\"{mid}_RAW.txt\"))\n","    print(\"Saved:\", os.path.join(out_dir, f\"{mid}_SANITIZED.txt\"))\n","    return report_san\n","\n","# ---- run for the held-out match ----\n","final_text = infer_for_match(TEST_ID, max_chunks=4, save=True)\n","print(\"\\n=== PREVIEW ===\\n\")\n","for i, p in enumerate([q for q in final_text.split(\"\\n\\n\") if q.strip()][:3]):\n","    print(p)\n","    if i < 2: print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qoo7KAL7NUGa","executionInfo":{"status":"ok","timestamp":1755556695921,"user_tz":-120,"elapsed":231423,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"b06043bb-59d7-43d3-8a36-2e6bc9ba72a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: /content/drive/MyDrive/model_weights_tokens_files/mistral7b_fp16_noshard_20250818-221113/reports_gen/1422119_RAW.txt\n","Saved: /content/drive/MyDrive/model_weights_tokens_files/mistral7b_fp16_noshard_20250818-221113/reports_gen/1422119_SANITIZED.txt\n","\n","=== PREVIEW ===\n","\n","Chennai Super Kings defeated Royal Challengers Bengaluru by 6 wickets at MA Chidambaram Stadium, Chepauk, Chennai.\n","\n","The opener's 48-ball 48 was the cornerstone of RCB's 132-run opening stand. But CSK's bowlers struck back with regular intervals. The Fizz, in particular, was on fire. He dismissed Kohli, Maxwell and Patidar in quick succession. The latter two were golden ducks. The pressure built up on the RCB batsmen, and they could not cope with it.\n","The middle order failed to capitalise on the start provided by the openers. Karthick and Ravindran scored 38 and 37 respectively, but they could not accelerate. The required rate kept climbing, and RCB lost wickets at regular intervals. CSK' s bowlers kept striking back. Green and Sharma picked up two wickets each. The pressure was too much for RCB to handle. They could only manage 157-9 in their 20 overs. CSK\n","\n"]}]},{"cell_type":"code","source":["# Cell X — Evaluate a generated summary (facts + style + ROUGE-L)\n","# Purpose: quick automatic checks against scorecard facts and a gold report (if available).\n","\n","import os, re, json\n","\n","# --------- utilities (lightweight, self-contained) ----------\n","def contains_literal(text, value):\n","    if not value: return True\n","    t = re.sub(r\"\\s+\", \" \", text or \"\").lower()\n","    v = re.sub(r\"\\s+\", \" \", str(value)).lower()\n","    return v in t\n","\n","def lcs_len(a, b):\n","    n, m = len(a), len(b)\n","    dp = [ [0]*(m+1) for _ in range(n+1) ]\n","    for i in range(1, n+1):\n","        ai = a[i-1]\n","        dpi = dp[i]\n","        dpim1 = dp[i-1]\n","        for j in range(1, m+1):\n","            dpi[j] = dpim1[j-1] + 1 if ai == b[j-1] else (dpi[j-1] if dpi[j-1] > dpim1[j] else dpim1[j])\n","    return dp[n][m]\n","\n","def rouge_l(pred, ref):\n","    p = (pred or \"\").split()\n","    r = (ref or \"\").split()\n","    if not p or not r:\n","        return {\"r\": 0.0, \"p\": 0.0, \"f\": 0.0}\n","    lcs = lcs_len(p, r)\n","    rec = lcs / len(r)\n","    prec = lcs / len(p)\n","    f = 0.0 if (rec + prec) == 0 else (2 * prec * rec) / (prec + rec + 1e-12)\n","    return {\"r\": round(rec, 4), \"p\": round(prec, 4), \"f\": round(f, 4)}\n","\n","def loser_of(stats):\n","    t1, t2, w = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\"), stats.get(\"winner\",\"\")\n","    return t2 if w and w == t1 else (t1 if w else \"\")\n","\n","def format_margin(stats):\n","    margin = stats.get(\"result_margin\")\n","    res_txt = (stats.get(\"result\") or \"\").lower()\n","    if margin in (None, \"\"): return \"\"\n","    try:\n","        n = int(margin)\n","        unit = \"wickets\" if \"wicket\" in res_txt else (\"runs\" if \"run\" in res_txt else \"\")\n","        return f\"{n} {unit}\".strip()\n","    except Exception:\n","        return str(margin)\n","\n","BALL_BY_BALL_PATTERNS = [\n","    r\"\\b(first|second|third|fourth|fifth|sixth)\\s+ball\\b\",\n","    r\"\\bnext ball\\b\", r\"\\bover\\-by\\-over\\b\", r\"\\bball\\-by\\-ball\\b\",\n","    r\"\\bcaught at\\b\", r\"\\byork(ed|er)\\b\", r\"\\brun out\\b\", r\"\\bstumped\\b\",\n","    r\"\\bslower ball\\b\", r\"\\b(\\d+)(th)?\\s+over\\b\",\n","]\n","\n","NAME_RX = re.compile(r\"[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\")\n","_GENERIC_BAD = {\"Chennai\",\"Super\",\"Kings\",\"Royal\",\"Challengers\",\"Bengaluru\",\"Bangalore\",\n","                \"Stadium\",\"Chepauk\",\"Match\",\"Overs\",\"Runs\",\"Wickets\",\"Powerplay\",\"Review\",\"DRS\"}\n","\n","def extract_names_from_text(txt):\n","    cands = [m.group(0).strip() for m in NAME_RX.finditer(txt or \"\")]\n","    return [c for c in cands if c not in _GENERIC_BAD and len(c) <= 30]\n","\n","def build_whitelist(stats):\n","    wl = set()\n","    for key in (\"top_batters\",\"top_bowlers\",\"players_t1\",\"players_t2\",\n","                \"batting_card_t1\",\"batting_card_t2\",\"bowling_card_t1\",\"bowling_card_t2\"):\n","        for item in (stats or {}).get(key, []) or []:\n","            if isinstance(item, dict):\n","                for k in (\"name\",\"player\",\"batter\",\"bowler\"):\n","                    if item.get(k): wl.add(str(item[k]).strip())\n","            elif isinstance(item, str):\n","                wl.add(item.strip())\n","    return {w.lower() for w in wl}\n","\n","# --------- load inputs ---------\n","mid = globals().get(\"TEST_ID\", \"1422119\")\n","save_dir = globals().get(\"SAVE_DIR_RUN\", None)\n","assert save_dir, \"SAVE_DIR_RUN not found (run training cell that sets it).\"\n","\n","# Prefer SANITIZED; fall back to RAW if needed\n","p_san = os.path.join(save_dir, \"reports_gen\", f\"{mid}_SANITIZED.txt\")\n","p_raw = os.path.join(save_dir, \"reports_gen\", f\"{mid}_RAW.txt\")\n","with open(p_san if os.path.exists(p_san) else p_raw, \"r\", encoding=\"utf-8\") as f:\n","    pred_text = f.read()\n","\n","# scorecard + (optional) gold\n","assert \"scorecards_by_id\" in globals(), \"Run the data-loading cell (Cell 7).\"\n","stats = scorecards_by_id[mid][\"stats\"]\n","gold_text = None\n","if \"reports_by_id\" in globals() and mid in reports_by_id:\n","    gold_text = reports_by_id[mid].get(\"report_text\", None)\n","\n","# --------- factual checks ---------\n","facts_ok = {\n","    \"winner\": contains_literal(pred_text, stats.get(\"winner\",\"\")),\n","    \"loser\":  contains_literal(pred_text, loser_of(stats)) if loser_of(stats) else True,\n","    \"margin_text\": contains_literal(pred_text, format_margin(stats)),\n","    \"venue\":  contains_literal(pred_text, stats.get(\"venue\",\"\")),\n","    \"toss\":   (contains_literal(pred_text, stats.get(\"toss_winner\",\"\")) and\n","               contains_literal(pred_text, stats.get(\"toss_decision\",\"\"))),\n","}\n","facts_ok[\"all_pass\"] = all(facts_ok.values())\n","\n","# --------- style & safety checks ---------\n","ball_by_ball_hits = sum(bool(re.search(p, pred_text, flags=re.I)) for p in BALL_BY_BALL_PATTERNS)\n","names_in_text = extract_names_from_text(pred_text)\n","whitelist = build_whitelist(stats)\n","oov_names = [n for n in names_in_text if n.lower() not in whitelist]\n","\n","style = {\n","    \"chars\": len(pred_text),\n","    \"sentences\": len(re.split(r'(?<=[.!?])\\s+', pred_text.strip())) if pred_text.strip() else 0,\n","    \"ball_by_ball_flags\": ball_by_ball_hits,\n","    \"names_total\": len(names_in_text),\n","    \"names_oov\": len(oov_names),\n","    \"oov_samples\": oov_names[:5],\n","}\n","\n","# --------- ROUGE-L (if gold exists) ---------\n","rouge = None\n","if gold_text:\n","    rouge = rouge_l(pred_text, gold_text)\n","\n","# --------- display ---------\n","print(\"MATCH_ID:\", mid)\n","print(\"\\nFACTS:\", facts_ok)\n","print(\"\\nSTYLE:\", style)\n","if rouge:\n","    print(\"\\nROUGE-L:\", rouge)\n","else:\n","    print(\"\\nROUGE-L: (skipped — no gold text available for this match)\")\n","\n","# Small excerpt print for human glance\n","print(\"\\n--- PRED EXCERPT ---\")\n","print(pred_text[:600].replace(\"\\n\",\" \") + (\"...\" if len(pred_text) > 600 else \"\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TW6wTsvEQRcw","executionInfo":{"status":"ok","timestamp":1755557069720,"user_tz":-120,"elapsed":48,"user":{"displayName":"Darshan Chaudhari","userId":"10553189373750965827"}},"outputId":"9aede001-7c28-4e93-8d94-b11567643546"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MATCH_ID: 1422119\n","\n","FACTS: {'winner': True, 'loser': True, 'margin_text': True, 'venue': True, 'toss': True, 'all_pass': True}\n","\n","STYLE: {'chars': 867, 'sentences': 15, 'ball_by_ball_flags': 0, 'names_total': 20, 'names_oov': 20, 'oov_samples': ['Chennai Super Kings', 'Royal Challengers Bengaluru', 'Chidambaram Stadium', 'The', 'But']}\n","\n","ROUGE-L: {'r': 0.1791, 'p': 0.1667, 'f': 0.1727}\n","\n","--- PRED EXCERPT ---\n","Chennai Super Kings defeated Royal Challengers Bengaluru by 6 wickets at MA Chidambaram Stadium, Chepauk, Chennai.  The opener's 48-ball 48 was the cornerstone of RCB's 132-run opening stand. But CSK's bowlers struck back with regular intervals. The Fizz, in particular, was on fire. He dismissed Kohli, Maxwell and Patidar in quick succession. The latter two were golden ducks. The pressure built up on the RCB batsmen, and they could not cope with it. The middle order failed to capitalise on the start provided by the openers. Karthick and Ravindran scored 38 and 37 respectively, but they could n...\n"]}]},{"cell_type":"code","source":["# Cell 10 — Inference that always completes (delayed stop + safe auto-continue)\n","# Purpose: guarantee a full two-paragraph summary; never end mid-thought.\n","\n","import os, re, json, torch\n","from transformers import (\n","    StoppingCriteria, StoppingCriteriaList, LogitsProcessorList,\n","    NoBadWordsLogitsProcessor, InfNanRemoveLogitsProcessor\n",")\n","\n","assert \"SAVE_DIR_RUN\" in globals(), \"Run training/saving (Cell 9) first.\"\n","assert \"commentary_by_id\" in globals() and \"scorecards_by_id\" in globals(), \"Run data load (Cell 7) first.\"\n","TEST_ID = globals().get(\"TEST_ID\", \"1422119\")\n","\n","# -------- helpers --------\n","def loser_of(stats):\n","    t1, t2, w = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\"), stats.get(\"winner\",\"\")\n","    return t2 if w == t1 else t1\n","\n","def format_margin(stats):\n","    margin = stats.get(\"result_margin\"); res_txt=(stats.get(\"result\") or \"\").lower()\n","    if margin in (None,\"\"): return \"\"\n","    try:\n","        n=int(margin); unit=\"wickets\" if \"wicket\" in res_txt else (\"runs\" if \"run\" in res_txt else \"\")\n","        return f\"{n} {unit}\".strip()\n","    except: return str(margin)\n","\n","def opening_sentence(stats):\n","    w=stats.get(\"winner\",\"\"); l=loser_of(stats); m=format_margin(stats); v=stats.get(\"venue\",\"\")\n","    return f\"{w} defeated {l} by {m} at {v}.\" if (w and l and m and v) else \"\"\n","\n","def compact_stats(stats):\n","    g=lambda o,k,d=None:(o or {}).get(k,d)\n","    return {\n","        \"team1\":g(stats,\"team1\",\"\"), \"team2\":g(stats,\"team2\",\"\"),\n","        \"winner\":g(stats,\"winner\",\"\"), \"result\":g(stats,\"result\",\"\"),\n","        \"result_margin\":g(stats,\"result_margin\",\"\"), \"venue\":g(stats,\"venue\",\"\"),\n","        \"toss_winner\":g(stats,\"toss_winner\",\"\"), \"toss_decision\":g(stats,\"toss_decision\",\"\"),\n","        \"top_batters\":g(stats,\"top_batters\",[]), \"top_bowlers\":g(stats,\"top_bowlers\",[]),\n","        \"players_t1\":g(stats,\"players_t1\",[]), \"players_t2\":g(stats,\"players_t2\",[]),\n","        \"batting_card_t1\":g(stats,\"batting_card_t1\",[]), \"batting_card_t2\":g(stats,\"batting_card_t2\",[]),\n","        \"bowling_card_t1\":g(stats,\"bowling_card_t1\",[]), \"bowling_card_t2\":g(stats,\"bowling_card_t2\",[]),\n","    }\n","\n","def pick_chunks(chunks, k=3): return (chunks or [])[:k]\n","\n","def clean_headings(txt):\n","    txt=re.sub(r\"(?m)^\\s*(P\\d:|Paragraph\\s*\\d:|Pargraph\\s*\\d:)\\s*\",\"\",txt)\n","    txt=re.sub(r\"\\s+\\.\",\".\",txt); txt=re.sub(r\"\\s+,\",\",\",txt)\n","    return txt.strip()\n","\n","def contains_literal(text, value):\n","    if not value: return True\n","    t=re.sub(r\"\\s+\",\" \", text or \"\").lower()\n","    v=re.sub(r\"\\s+\",\" \", str(value)).lower()\n","    return v in t\n","\n","# --- soft name handling (no line drops) ---\n","NAME_RX = re.compile(r\"[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\")\n","def extract_names(txt): return [m.group(0) for m in NAME_RX.finditer(txt or \"\")]\n","\n","def build_whitelist(stats):\n","    wl=set()\n","    for key in (\"top_batters\",\"top_bowlers\",\"players_t1\",\"players_t2\",\n","                \"batting_card_t1\",\"batting_card_t2\",\"bowling_card_t1\",\"bowling_card_t2\"):\n","        for it in (stats or {}).get(key, []) or []:\n","            if isinstance(it, dict):\n","                for k in (\"name\",\"player\",\"batter\",\"bowler\"):\n","                    if it.get(k): wl.add(str(it[k]).strip())\n","            elif isinstance(it, str):\n","                wl.add(it.strip())\n","    # also allow team names, venue & common aliases\n","    for k in (\"team1\",\"team2\",\"venue\"):\n","        v=(stats or {}).get(k,\"\")\n","        if v: wl.add(v)\n","    t1, t2 = stats.get(\"team1\",\"\"), stats.get(\"team2\",\"\")\n","    if \"Chennai Super Kings\" in (t1+t2): wl.update({\"CSK\",\"Chennai\"})\n","    if \"Royal Challengers\" in (t1+t2): wl.update({\"RCB\",\"Bengaluru\",\"Bangalore\"})\n","    return {w.lower():w for w in wl}\n","\n","def sanitize_names_soft(text, wl_map):\n","    out=text\n","    for n in set(extract_names(text)):\n","        rep = wl_map.get(n.lower())\n","        if rep and rep != n:\n","            out = re.sub(rf\"\\b{re.escape(n)}\\b\", rep, out)\n","    # fix a couple of annoying artifacts\n","    out = out.replace(\"Player response\", \"In response\")\n","    out = out.replace(\"Player (IPL)\", \"the IPL\")\n","    return out\n","\n","# --- keep only egregious ball-by-ball phrases out ---\n","BB_PATTERNS = [r\"\\bball\\-by\\-ball\\b\", r\"\\bover\\-by\\-over\\b\"]\n","def strip_ball_by_ball(text):\n","    sents = re.split(r'(?<=[.!?])\\s+', text)\n","    kept=[s for s in sents if not any(re.search(p, s, flags=re.I) for p in BB_PATTERNS)]\n","    return \" \".join(kept).strip()\n","\n","def enforce_two_paragraphs(text):\n","    paras=[p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n","    if len(paras)>=3: return \"\\n\\n\".join(paras[:3])\n","    if len(paras)==1:\n","        sents=re.split(r'(?<=[.!?])\\s+', paras[0])\n","        cut=max(2, len(sents)//2)\n","        return ( \" \".join(sents[:cut]).strip() + \"\\n\\n\" + \" \".join(sents[cut:]).strip() ).strip()\n","    return \"\\n\\n\".join(paras)\n","\n","# --- decoding control: delayed stop on <END> ---\n","class StopOnTokensAfterMin(StoppingCriteria):\n","    def __init__(self, stop_ids, prompt_len, min_new_tokens):\n","        self.stop_ids=stop_ids; self.prompt_len=prompt_len; self.min_new_tokens=min_new_tokens\n","    def __call__(self, input_ids, scores, **kwargs)->bool:\n","        gen_len = input_ids.shape[-1]-self.prompt_len\n","        if gen_len < self.min_new_tokens: return False\n","        S=self.stop_ids.shape[-1]\n","        if input_ids.shape[-1] < S: return False\n","        return torch.equal(input_ids[0,-S:].cpu(), self.stop_ids.cpu())\n","\n","def make_stop_after_min(tokenizer, stop_str, prompt_len, min_new_tokens):\n","    stop_ids = tokenizer(stop_str, add_special_tokens=False, return_tensors=\"pt\").input_ids[0]\n","    return StoppingCriteriaList([StopOnTokensAfterMin(stop_ids, prompt_len, min_new_tokens)])\n","\n","def build_bad_words_ids(tokenizer, words):\n","    ids=[];\n","    for w in words:\n","        toks=tokenizer(w, add_special_tokens=False).input_ids\n","        if toks: ids.append(toks)\n","    return ids\n","\n","def contradiction_phrases(stats):\n","    res=(stats.get(\"result\") or \"\").lower()\n","    if \"wicket\" in res:  # winner chased\n","        return [\"fell short\",\"could not chase\",\"defended the total\",\"won by runs\",\"victory by runs\",\"ran out of overs\"]\n","    if \"run\" in res:     # winner defended\n","        return [\"won by wickets\",\"victory by wickets\",\"chased down comfortably\",\"reached the target\",\"got over the line in the chase\"]\n","    return []\n","\n","# --- prompting & generation ---\n","def build_prompt(stats, chunks, wl_map):\n","    allowed = \", \".join(sorted(set(wl_map.values())))\n","    return (\n","        \"You are a cricket expert. Continue the match summary in exactly two short paragraphs (no headings).\\n\"\n","        \"The REQUIRED opening sentence has already been written. Do NOT repeat it.\\n\\n\"\n","        \"Rules:\\n\"\n","        \"- Max 6 sentences total across both paragraphs.\\n\"\n","        \"- Use only facts from SCORECARD and COMMENTARY EXCERPTS. Never invent.\\n\"\n","        \"- Summarise key overs/partnerships; do NOT narrate ball-by-ball.\\n\"\n","        f\"- If you mention players, use ONLY these names (or use roles like 'the opener'/'the seamer'): {allowed or '—'}\\n\"\n","        \"- End with <END>.\\n\\n\"\n","        f\"SCORECARD:\\n{json.dumps(stats, ensure_ascii=False)}\\n\\n\"\n","        \"COMMENTARY EXCERPTS:\\n\" + \"\\n\".join(chunks) + \"\\n\\n\"\n","        \"Continue with the two paragraphs only.\"\n","    )\n","\n","def generate_once(prompt, min_nt=220, max_nt=340, banned_extra=None):\n","    enc = tok(prompt, return_tensors=\"pt\")\n","    input_ids = enc.input_ids.to(model.device)\n","    attention_mask = enc.attention_mask.to(model.device)\n","\n","    banned = [\"P1:\",\"P2:\",\"Paragraph 1:\",\"Paragraph 2:\",\"Section\",\"Heading\",\"Outline:\"]\n","    if banned_extra: banned += banned_extra\n","    bad_words_ids = build_bad_words_ids(tok, banned)\n","\n","    stop_crit = make_stop_after_min(tok, \"<END>\", prompt_len=input_ids.shape[-1], min_new_tokens=min_nt)\n","    logits_processors = LogitsProcessorList([\n","        InfNanRemoveLogitsProcessor(),\n","        NoBadWordsLogitsProcessor(bad_words_ids=bad_words_ids, eos_token_id=tok.eos_token_id),\n","    ])\n","\n","    gen_kwargs = dict(\n","        do_sample=False,\n","        repetition_penalty=1.02,\n","        no_repeat_ngram_size=4,\n","        min_new_tokens=min_nt,\n","        max_new_tokens=max_nt,\n","        pad_token_id=tok.pad_token_id,\n","        use_cache=True,\n","        eos_token_id=None,  # stop only on <END>\n","        logits_processor=logits_processors,\n","        stopping_criteria=stop_crit,\n","    )\n","\n","    with torch.no_grad():\n","        out_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_kwargs)\n","\n","    prompt_len = input_ids.shape[-1]\n","    cont_ids = out_ids[0, prompt_len:]\n","    text = tok.decode(cont_ids, skip_special_tokens=True)\n","    # Return up to <END> if present; else full continuation\n","    return text.split(\"<END>\")[0].strip() if \"<END>\" in text else text.strip()\n","\n","def continue_once(opener, partial):\n","    # Finish cleanly with one extra sentence and <END>\n","    cont_prompt = (\n","        \"You are a cricket expert. The summary stopped early. \"\n","        \"Write ONE closing sentence that wraps up why the result happened. Then output <END>.\\n\\n\"\n","        f\"OPENING:\\n{opener}\\n\\n\"\n","        f\"SUMMARY SO FAR:\\n{partial}\\n\\n\"\n","        \"One sentence only. End with <END>.\"\n","    )\n","    add = generate_once(cont_prompt, min_nt=12, max_nt=40, banned_extra=[])\n","    if \"<END>\" in add: add = add.split(\"<END>\")[0].strip()\n","    if not add.endswith((\".\", \"!\", \"?\")): add += \".\"\n","    # append to paragraph 2\n","    if \"\\n\\n\" in partial:\n","        return (partial.rstrip() + \" \" + add).strip()\n","    # if model returned one long para, split then append\n","    sents = re.split(r'(?<=[.!?])\\s+', partial)\n","    cut = max(2, len(sents)//2)\n","    return (\" \".join(sents[:cut]).strip() + \"\\n\\n\" + \" \".join(sents[cut:]).strip() + \" \" + add).strip()\n","\n","def infer_for_match(mid, max_chunks=3, save=True):\n","    com = commentary_by_id[mid]; sc = scorecards_by_id[mid]\n","    stats = compact_stats(sc[\"stats\"])\n","    chunks = pick_chunks(com.get(\"commentary_chunks\", []), k=max_chunks)\n","    wl_map = build_whitelist(stats)\n","\n","    prompt = build_prompt(stats, chunks, wl_map)\n","    banned_extra = contradiction_phrases(stats)\n","\n","    body = generate_once(prompt, min_nt=220, max_nt=340, banned_extra=banned_extra)\n","    opener = opening_sentence(stats)\n","    body = clean_headings(body)\n","\n","    if \"<END>\" not in body:\n","        body = continue_once(opener, body)\n","\n","    # Assemble & soft-clean\n","    raw_report = (opener + (\"\\n\\n\" if opener else \"\") + body).strip()\n","    san = sanitize_names_soft(raw_report, wl_map)\n","    san = strip_ball_by_ball(san)\n","    final_report = enforce_two_paragraphs(san)\n","\n","    # guardrail: keep crucial literals\n","    need = [stats.get(\"winner\",\"\"), format_margin(stats), stats.get(\"venue\",\"\")]\n","    if not all(contains_literal(final_report, x) for x in need):\n","        if not final_report.endswith((\".\", \"!\", \"?\")): final_report += \".\"\n","        final_report += f\" The result reflected decisive spells and partnerships at {stats.get('venue','the venue')}.\"\n","\n","    # Save RAW / SANITIZED / FINAL\n","    out_dir = os.path.join(SAVE_DIR_RUN, \"reports_gen\"); os.makedirs(out_dir, exist_ok=True)\n","    base = os.path.join(out_dir, f\"match_{mid}\")\n","    with open(base + \"_RAW.txt\", \"w\", encoding=\"utf-8\") as f: f.write(raw_report)\n","    with open(base + \"_SANITIZED.txt\", \"w\", encoding=\"utf-8\") as f: f.write(final_report)\n","    with open(base + \".txt\", \"w\", encoding=\"utf-8\") as f: f.write(final_report)\n","    print(\"Saved:\", base + \"_RAW.txt\")\n","    print(\"Saved:\", base + \"_SANITIZED.txt\")\n","    print(\"Saved:\", base + \".txt\")\n","    return final_report\n","\n","# Run once for the held-out match (prints full final report)\n","final_text = infer_for_match(TEST_ID, max_chunks=3, save=True)\n","print(\"\\n=== PREVIEW ===\\n\")\n","print(final_text)"],"metadata":{"id":"w_apjKyCWwFU"},"execution_count":null,"outputs":[]}]}